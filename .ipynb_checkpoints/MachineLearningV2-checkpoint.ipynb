{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L7m7KcsSWdnR"
   },
   "outputs": [],
   "source": [
    "#3rd party libraries\n",
    "#!pip install pandas_bokeh\n",
    "#!pip install bayesian-optimization\n",
    "#!pip install six\n",
    "#!pip install eli5\n",
    "#!pip install yellowbrick\n",
    "#!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GXnMFrQ-NT5A"
   },
   "outputs": [],
   "source": [
    "#yellowbrick\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV \n",
    "from sklearn.linear_model import Perceptron, LogisticRegressionCV, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score,mean_absolute_error, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc, classification_report,precision_score,recall_score,log_loss,f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, LabelEncoder, OneHotEncoder, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer,minmax_scale\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import tree\n",
    "import pandas_bokeh\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from numpy import mean, std\n",
    "import pandas.testing as tm\n",
    "from scipy import stats\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#other\n",
    "from math import sqrt\n",
    "\n",
    "import eli5\n",
    "\n",
    "from yellowbrick.features import Rank2D\n",
    "\n",
    "from yellowbrick.features.radviz import RadViz\n",
    "from yellowbrick.features import pca_decomposition\n",
    "from yellowbrick.features import Manifold\n",
    "from yellowbrick.features import JointPlotVisualizer\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from yellowbrick.features import ParallelCoordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ckMkNrcNVg0",
    "outputId": "a5698974-ea0d-4299-d78d-56d181beb8e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2n Cycle' 'Basic' 'Graduation' 'Master' 'PhD']\n",
      "Index(['Year_Birth', 'Education', 'Income', 'Kidhome', 'Teenhome', 'Recency',\n",
      "       'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
      "       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
      "       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
      "       'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5',\n",
      "       'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Marital_Absurd',\n",
      "       'Marital_Alone', 'Marital_Divorced', 'Marital_Married',\n",
      "       'Marital_Single', 'Marital_Together', 'Marital_Widow', 'Marital_YOLO',\n",
      "       'enroll_year', 'enroll_month', 'enroll_day'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "DATASET_URL = \"https://gist.githubusercontent.com/YHYeoh/ad1a7f7170c72d621d05a70637540152/raw/5a6059c199e2c46d2f3d258f03d93cfea98e2749/marketing_campaign.csv\"\n",
    "data = pd.read_csv(DATASET_URL, sep = ';')\n",
    "\n",
    "pd.set_option('plotting.backend','pandas_bokeh')\n",
    "\n",
    "data.fillna(method = \"ffill\", inplace = True)\n",
    "data.isnull().values.any()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "enc = OneHotEncoder()\n",
    "data[\"Education\"] = label_encoder.fit_transform(data[\"Education\"])\n",
    "print(label_encoder.classes_)\n",
    "# enc_df = pd.DataFrame(enc.fit_transform(data[[\"Marital_Status\"]]).toarray())\n",
    "# print(enc.get_feature_names())\n",
    "# data = data.join(enc_df)\n",
    "\n",
    "data = pd.concat([data, pd.get_dummies(data[\"Marital_Status\"],prefix=\"Marital\")], axis=1)\n",
    "\n",
    "data['enroll_year'] = pd.DatetimeIndex(data.Dt_Customer).year\n",
    "data['enroll_month'] = pd.DatetimeIndex(data.Dt_Customer).month\n",
    "data['enroll_day'] = pd.DatetimeIndex(data.Dt_Customer).day\n",
    "\n",
    "data.drop([\"ID\", 'Dt_Customer',\"Z_CostContact\",\"Z_Revenue\",\"Marital_Status\"], axis=1, inplace=True)\n",
    "\n",
    "categorical = ['Marital_Status']\n",
    "numerical = ['Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n",
    "       'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n",
    "       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\n",
    "       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',\n",
    "       'Complain', 'enroll_year', 'enroll_month', 'enroll_day']\n",
    "numerical_no_bool = ['Education','Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth','enroll_day','enroll_month','enroll_year']\n",
    "\n",
    "y = data.Response\n",
    "X = data.drop(['Response'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.3)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LkPBcz5KNZi6"
   },
   "outputs": [],
   "source": [
    "def hasmethod(obj, name):\n",
    "    return inspect.ismethod(getattr(obj, name, None))\n",
    "\n",
    "def ROC_Curve_Plot(model,X_test,y_test):\n",
    "    predProb = model.predict_proba(X_test)\n",
    "    preds = predProb[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds,pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.close()\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    #plt.xlim([0, 1])\n",
    "    #plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    return fpr,tpr\n",
    "\n",
    "def setupPreprocessPipeline(scaler):\n",
    "\tss = Pipeline(steps=[('scaler',scaler)])\n",
    "\t#ohe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "\tpreprocess = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('cont', ss, numerical_no_bool),\n",
    "                        #('cat', categorical),\n",
    "                        #('le', le, ordinal),\n",
    "                        ],remainder='passthrough')\n",
    "\treturn preprocess\n",
    "\n",
    "def feature_importance(classifier, feature_names, scaler_name):\n",
    "\tif (hasattr(classifier,'coef_')):\n",
    "\t\timportance = classifier.coef_[0]\n",
    "\telif (hasattr(classifier,'coefs_')):\n",
    "\t\timportance = classifier.coefs_\n",
    "\telif (hasattr(classifier,'feature_importances_')):\n",
    "\t\timportance = classifier.feature_importances_\n",
    "\telse:\n",
    "\t\tprint(\"Cannot extract feature importance, skipping\")\n",
    "\t\treturn\n",
    "\n",
    "\tfor i,v in enumerate(importance):\n",
    "\t\tprint('Feature: %d, Score: %.5f' % (i,v))\n",
    "\tzipped = zip(feature_names, importance)\n",
    "\tdf = pd.DataFrame(zipped, columns=[\"feature\", \"value\"])\n",
    "\t# Sort the features by the absolute value of their coefficient\n",
    "\tdf[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\n",
    "\tdf[\"colors\"] = df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\n",
    "\tdf = df.sort_values(\"abs_value\", ascending=False)\n",
    "\t# plot feature importance\n",
    "\tfig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "\tsns.barplot(x=\"feature\",\n",
    "\t            y=\"value\",\n",
    "\t            data=df.head(20),\n",
    "\t           palette=df.head(20)[\"colors\"])\n",
    "\tplt.gcf().subplots_adjust(bottom=0.30)\n",
    "\tax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=14)\n",
    "\tax.set_title(\"Top 20 Features for {} w/ {}\".format(classifier.__class__.__name__, scaler_name), fontsize=25)\n",
    "\tax.set_ylabel(\"Coef\", fontsize=22)\n",
    "\tax.set_xlabel(\"Feature Name\", fontsize=22)\n",
    "\tplt.show()\n",
    "\n",
    "def evaluation(y, y_hat, title):\n",
    "    cm = confusion_matrix(y, y_hat)\n",
    "    precision = precision_score(y, y_hat)\n",
    "    recall = recall_score(y, y_hat)\n",
    "    accuracy = accuracy_score(y,y_hat)\n",
    "    f1 = f1_score(y,y_hat)\n",
    "    print('Recall: ', recall)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('Precision: ', precision)\n",
    "    print('F1: ', f1)\n",
    "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\n",
    "    plt.xlabel('predicted', fontsize=18)\n",
    "    plt.ylabel('actual', fontsize=18)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "def metrics_summary(y_test,y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall =  recall_score(y_test, y_pred) #sensitivity\n",
    "    specificity = tn / (tn+fp)\n",
    "    g_mean= sqrt(recall * specificity)\n",
    "    mse =mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2=r2_score(y_test, y_pred)\n",
    "    ros = roc_auc_score(y_test, y_pred)\n",
    "    ll = log_loss(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    metrics_collection_dict ={\n",
    "        'accuracy':['accuracy',accuracy],\n",
    "        'precision':['precision',precision],\n",
    "        'recall':['recall',recall],\n",
    "        'specificity':['specificity',specificity],\n",
    "        'g_mean':['g_mean',g_mean],\n",
    "        'mean_square_error':['mean_square_error',mse],\n",
    "        'r2':['r2',r2],\n",
    "        'roc_auc_score':['roc_auc_score',ros],\n",
    "        'log_loss':['log_loss',ll],\n",
    "        'f1_score':['f1_score',f1]\n",
    "    } \n",
    "    return metrics_collection_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "TxAAHzryNskR",
    "outputId": "ec64e357-f8ec-4d12-e1aa-e0735ef59f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-01d273c02ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mmodel_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-01d273c02ad9>\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(classifier, cv)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtrain_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                         \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                         \u001b[0my_hat_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                         \u001b[0mtrain_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m                 **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;31m# set n_features_in_ attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_validate_transformers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[1;31m# validate names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "def cross_validate(classifier, cv):\n",
    "\tscalers = [StandardScaler(),MinMaxScaler(),MaxAbsScaler(), RobustScaler()]\n",
    "\ttrain_acc = []\n",
    "\ttest_acc = []\n",
    "\tmean = []\n",
    "\tresult = []\n",
    "\tfor scaler in scalers:\n",
    "\t\tfpr = None\n",
    "\t\ttpr = None\n",
    "\t\tpreprocess = setupPreprocessPipeline(scaler)\n",
    "\t\tpipeline = Pipeline(steps=[\n",
    "\t        ('preprocess', preprocess),\n",
    "\t        ('classifier', classifier)\n",
    "    \t])\n",
    "\n",
    "\t\ttrain_acc = []\n",
    "\t\ttest_acc = []\n",
    "\t\tmean = []\n",
    "\n",
    "\t\tfor train_ind, val_ind in cv.split(X_train, y_train):\n",
    "\t\t\tX_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind]\n",
    "\t\t\tpipeline.fit(X_t, y_t)\n",
    "\t\t\ty_hat_t = pipeline.predict(X_t)\n",
    "\t\t\ttrain_acc.append(accuracy_score(y_t, y_hat_t))\n",
    "\t\t\tX_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind] \n",
    "\t\t\ty_hat_val = pipeline.predict(X_val)\n",
    "\t\t\ttest_acc.append(accuracy_score(y_val, y_hat_val))\n",
    "\n",
    "\t\t#ohe_cols = list(pipeline.named_steps['preprocess'].named_transformers_['cat'].named_steps['ohe'].get_feature_names(input_features=categorical))\n",
    "\t\t#numeric_feature_list = list(numerical)\n",
    "\t\t#for i in ohe_cols:\n",
    "\t\t#\tnumeric_feature_list.append(i)\n",
    "\t\t#print(len(numeric_feature_list))\n",
    "\t\tevaluation(y_val, y_hat_val, 'Confusion Matrix {} + {}'.format(classifier.__class__.__name__, scaler.__class__.__name__).strip())\n",
    "\t\tprint('Mean Training Accuracy: {} | Standard Deviation: {}'.format(np.mean(train_acc),np.std(test_acc)))\n",
    "\t\tprint('Mean Validation Accuracy: {} | Standard Deviation: {}'.format(np.mean(test_acc),np.std(test_acc)))\n",
    "\t\tprint('\\n')\n",
    "        \n",
    "\t\tfeature_importance(classifier, numeric_feature_list, scaler.__class__.__name__ )\n",
    "\t\tmetrics_summ = metrics_summary(y_val,y_hat_val)\n",
    "\t\tif hasmethod(pipeline['classifier'], 'predict_proba'):\n",
    "\t\t\tfpr,tpr = ROC_Curve_Plot(pipeline,X_val,y_val)\n",
    "\t\tresult.append({\n",
    "            'classifier':classifier.__class__.__name__,\n",
    "            'scalerName':scaler.__class__.__name__,\n",
    "            'metrics_summ':metrics_summ,\n",
    "            'fpr':fpr,\n",
    "            'tpr':tpr\n",
    "        })\n",
    "\treturn result\n",
    "\n",
    "models = [\n",
    "\tLasso(),\n",
    "\tSGDClassifier(max_iter = 1000, tol=1e-3,penalty = \"elasticnet\"),\n",
    "\tLinearSVC(), \n",
    "\tGaussianProcessClassifier(),\n",
    "\tExtraTreeClassifier(),\n",
    "\tBernoulliNB(),\n",
    "\tLogisticRegressionCV(max_iter= 1200), \n",
    "\tRidgeClassifier(),\n",
    "\tSVC(kernel = 'linear',max_iter= -1), \n",
    "\tPerceptron(),\n",
    "\tPassiveAggressiveClassifier(), \n",
    "\tDecisionTreeClassifier(), #no coef \n",
    "\tKNeighborsClassifier(),#no feat_import, use permutation_importance \n",
    "\tGaussianNB(), #no feat_import, use permutation_importance \n",
    "\tLGBMClassifier(),#no coef \n",
    "\tRandomForestClassifier(), #no coef \n",
    "\tGradientBoostingClassifier(),#no coef \n",
    "\tPassiveAggressiveClassifier(), \n",
    "\tExtraTreesClassifier(), #no coef \n",
    "\tXGBClassifier(),\n",
    "\tAdaBoostClassifier(), #no coef \n",
    "\t]\n",
    "\n",
    "model_result = []\n",
    "\n",
    "for model in models:\n",
    "\tprint(model.__class__.__name__)\n",
    "\tmodel_result.append(cross_validate(model,KFold()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yjPnG_fn3XJ"
   },
   "outputs": [],
   "source": [
    "standardScalerList = []\n",
    "minMaxScalerList = []\n",
    "maxAbsScalerList = []\n",
    "robustScalerList = []\n",
    "for collection in model_result:\n",
    "    standard = collection[0]\n",
    "    standardScalerList.append({'classifier':standard['classifier'],'metrics_summ':standard['metrics_summ'],'fpr':standard['fpr'],'tpr':standard['tpr'] })\n",
    "    minMax = collection[1]\n",
    "    minMaxScalerList.append({'classifier':minMax['classifier'],'metrics_summ':minMax['metrics_summ'],'fpr':minMax['fpr'],'tpr':minMax['tpr'] })\n",
    "    maxAbs = collection[2]\n",
    "    maxAbsScalerList.append({'classifier':maxAbs['classifier'],'metrics_summ':maxAbs['metrics_summ'],'fpr':maxAbs['fpr'],'tpr':maxAbs['tpr'] })\n",
    "    robust = collection[3]\n",
    "    robustScalerList.append({'classifier':robust['classifier'],'metrics_summ':robust['metrics_summ'],'fpr':robust['fpr'],'tpr':robust['tpr'] })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzI5diiIn3OT"
   },
   "outputs": [],
   "source": [
    "#plot all metric for all scaler\n",
    "nameList = []\n",
    "\n",
    "\n",
    "for object in model_result:\n",
    "    nameList.append(object[0]['classifier'])\n",
    "metric_list = ['accuracy','precision','recall','specificity','g_mean'\n",
    "                   ,'mean_square_error','r2','roc_auc_score','log_loss','f1_score']\n",
    "scaler = ['Standard Scaler','Min Max Scaler','Max Abs Scaler','robust Scaler']\n",
    "for metric in metric_list:\n",
    "    resultList = []\n",
    "    for model in model_result:\n",
    "        resultList.append(model[0]['metrics_summ'][metric][1])\n",
    "        \n",
    "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\n",
    "    plt.title(\"Models' \"+metric+ \" with Standard Scaler\")\n",
    "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VHwPTf6n3ER"
   },
   "outputs": [],
   "source": [
    "for metric in metric_list:\n",
    "    resultList = []\n",
    "    for model in model_result:\n",
    "        resultList.append(model[1]['metrics_summ'][metric][1])\n",
    "        \n",
    "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\n",
    "    plt.title(\"Models' \"+metric+ \" with Min Max Scaler\")\n",
    "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiDEyUDbn26U"
   },
   "outputs": [],
   "source": [
    "for metric in metric_list:\n",
    "    resultList = []\n",
    "    for model in model_result:\n",
    "        resultList.append(model[2]['metrics_summ'][metric][1])\n",
    "        \n",
    "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\n",
    "    plt.title(\"Models' \"+metric+ \" with Max Abs Scaler\")\n",
    "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSsQdWSyn2gc"
   },
   "outputs": [],
   "source": [
    "\n",
    "for metric in metric_list:\n",
    "    resultList = []\n",
    "    for model in model_result:\n",
    "        resultList.append(model[3]['metrics_summ'][metric][1])\n",
    "        \n",
    "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\n",
    "    plt.title(\"Models' \"+metric+ \" with Robust Scaler\")\n",
    "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeGnWf1goXwc"
   },
   "outputs": [],
   "source": [
    "scaler = ['Standard Scaler','Min Max Scaler','Max Abs Scaler','robust Scaler']\n",
    "scalerIndex = [0,1,2,3]\n",
    "\n",
    "for scalerIndex, scalerName in zip(scalerIndex,scaler):\n",
    "    result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "    for model in model_result:\n",
    "\n",
    "        fpr = model[scalerIndex]['fpr']\n",
    "        if(fpr is not None):\n",
    "            tpr = model[scalerIndex]['tpr']\n",
    "            auc = model[scalerIndex]['metrics_summ']['roc_auc_score'][1]\n",
    "            result_table = result_table.append({'classifiers':model[scalerIndex]['classifier'],\n",
    "                                            'fpr':fpr, \n",
    "                                            'tpr':tpr, \n",
    "                                            'auc':auc}, ignore_index=True)\n",
    "    result_table.set_index('classifiers', inplace=True)\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size('large')\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    for i in result_table.index:\n",
    "        plt.plot(result_table.loc[i]['fpr'], \n",
    "                 result_table.loc[i]['tpr'], \n",
    "                 label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "    plt.title(scalerName+', ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "    plt.legend( title='Models', bbox_to_anchor=(1.05, 0.85), loc='upper left', prop=fontP)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6Kj_TKMm84g"
   },
   "outputs": [],
   "source": [
    "#gridsearchcv params\n",
    "scoring = 'f1'\n",
    "fold=10\n",
    "featureNumList = list(range(1,X_train.shape[1]))\n",
    "modelsWithParam = [\n",
    "    {\n",
    "        'model':GradientBoostingClassifier(),'param': {\n",
    "        'loss':['deviance', 'exponential'],\n",
    "        'learning_rate': [0.1,0.01,0.005,0.001,0.0005,0.0001],\n",
    "        'min_samples_split': [0.1, 0.5, 5.0,10.0,20.0,40.0,80.0],\n",
    "        'min_samples_leaf': [0.1, 0.5, 5,10,20,40,80],\n",
    "        'max_depth':[5,10, 20, 30, 40, 60, 100, None],\n",
    "        'max_features':featureNumList,\n",
    "        'subsample':['0.8','1','1.2','1.4','1.6'],\n",
    "        'random_state': [42]\n",
    "        }}\n",
    "    ,{\n",
    "        'model':RandomForestClassifier(),'param':{'bootstrap': [True, False],\n",
    "        'max_depth': [5,10, 20, 30, 40, 60, 100, None],\n",
    "        'max_features': featureNumList,\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'min_samples_split': [0.1, 0.5, 5.0,10.0,20.0,40.0,80.0],\n",
    "        'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "        'random_state': [42]\n",
    "        }\n",
    "},{\n",
    "        'model':LGBMClassifier(),'param':{\n",
    "        'learning_rate': [0.1,0.01,0.005,0.001,0.0005,0.0001],\n",
    "        'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "        'max_depth': [5,10, 20, 30, 40, 60, 100, None],\n",
    "        'min_samples_split': [0.1, 0.5, 5.0,10.0,20.0,40.0,80.0],\n",
    "        'min_samples_leaf':[0.1, 0.5, 5,10,20,40,80],\n",
    "        'random_state': [42]\n",
    "    }},{\n",
    "            \n",
    "        'model':SVC(),'param':{\n",
    "        'C': [0.1, 1, 10, 100, 1000],  \n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "        'kernel': ['linear', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'random_state': [42] \n",
    "    }},\n",
    "    {\n",
    "        'model':SVC(),'param':{\n",
    "        'C': [0.1, 1, 10, 100, 1000],  \n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "        'kernel': ['poly'],\n",
    "        'degree':[3,4,5,6],\n",
    "        'random_state': [42] \n",
    "    }},{\n",
    "        'model':RidgeClassifier(),'param':{'alpha': [100,10,1, 0.1,0.05,0.001,0.0005],#100,10,1, 0.1, 0.01, 0.001\n",
    "                                 'fit_intercept':[True,False],\n",
    "                                 'normalize':[True,False],\n",
    "                                 'copy_X':[True],\n",
    "                                 'max_iter':[1,5,10,20,40,80,160,320,740,1480],#[50,100,500,1000,2000,4000,8000]\n",
    "                                 'solver':['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                                 'random_state':[42]\n",
    "        }}]\n",
    "fold = 10\n",
    "#gridsearchcv\n",
    "def modelBestFit(item):\n",
    "    model = item['model']\n",
    "    paramGrid = item['param']\n",
    "    search = GridSearchCV(model, paramGrid,n_jobs=-1,pre_dispatch='1*n_jobs',scoring = scoring,refit = True)\n",
    "    search.fit(X_train,y_train)\n",
    "    best_score =search.score(X_test,y_test)\n",
    "    model_name = model.__class__.__name__\n",
    "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\n",
    "\n",
    "def bestModel(modelsAndParams):\n",
    "    modelPerformance = pd.DataFrame()\n",
    "    for item in modelsAndParams:\n",
    "        result = modelBestFit(item)\n",
    "        \n",
    "        modelPerformance = modelPerformance.append(result,ignore_index=True)\n",
    "        \n",
    "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\n",
    "    return modelPerformance\n",
    "result = bestModel(modelsWithParam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoNS6Nn8m_cf"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlKtak7OuE9C"
   },
   "outputs": [],
   "source": [
    "#randomized search cv params\n",
    "scoring = 'f1'\n",
    "fold = 10\n",
    "featureNumList = list(range(1,X_train.shape[1]))\n",
    "modelsWithParam = [ {\n",
    "        'model':GradientBoostingClassifier(),'param': {\n",
    "        'loss':['deviance', 'exponential'],\n",
    "        'learning_rate': loguniform(1e-5, 100),\n",
    "        'min_samples_split': loguniform(1e-5, 100),\n",
    "        'min_samples_leaf': loguniform(1e-5, 100),\n",
    "        'max_depth':list(range(5,100)),\n",
    "        'max_features':featureNumList,\n",
    "        'subsample':loguniform(0.1, 2),\n",
    "        'random_state': [42]\n",
    "        }}\n",
    "    ,\n",
    "   {\n",
    "        'model':RandomForestClassifier(),'param':{'bootstrap': [True, False],\n",
    "        'max_depth': list(range(5,100)),\n",
    "        'max_features': featureNumList,\n",
    "        'min_samples_leaf': loguniform(1e-5, 100),\n",
    "        'min_samples_split': loguniform(1e-5, 100),\n",
    "        'n_estimators': list(range(1,5000)),\n",
    "        'random_state': [42]\n",
    "        }\n",
    "},{\n",
    "        'model':LGBMClassifier(),'param':{\n",
    "        'learning_rate': loguniform(1e-5, 100),\n",
    "        'n_estimators': list(range(1,200)),\n",
    "        'max_depth': list(range(5,100)),\n",
    "        'min_samples_split': loguniform(1e-5, 100),\n",
    "        'min_samples_leaf':loguniform(1e-5, 100),\n",
    "        'random_state': [42]\n",
    "    }},{\n",
    "            \n",
    "        'model':SVC(),'param':{\n",
    "        'C': loguniform(1e-5, 10),  \n",
    "        'gamma': loguniform(1e-5, 100), \n",
    "        'kernel': ['linear', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'random_state': [42] \n",
    "    }},\n",
    "    {\n",
    "        'model':SVC(),'param':{\n",
    "        'C': loguniform(1e-5, 100),  \n",
    "        'gamma': loguniform(1e-5, 100), \n",
    "        'kernel': ['poly'],\n",
    "        'degree':list(range(3,12)),\n",
    "        'random_state': [42] \n",
    "    }},{\n",
    "        'model':RidgeClassifier(),'param':{'alpha': loguniform(1e-5, 100),#100,10,1, 0.1, 0.01, 0.001\n",
    "                                 'fit_intercept':[True,False],\n",
    "                                 'normalize':[True,False],\n",
    "                                 'copy_X':[True],\n",
    "                                 'max_iter':list(range(1,5000)),#[50,100,500,1000,2000,4000,8000]\n",
    "                                 'solver':['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                                 'random_state':[42]\n",
    "        }}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flSSSMRZuO4g"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgZ4aq13xp6u"
   },
   "outputs": [],
   "source": [
    "#randomised search cv\n",
    "def modelBestFit(item):\n",
    "    model = item['model']\n",
    "    paramGrid = item['param']\n",
    "    search = RandomizedSearchCV(model, paramGrid,n_jobs=-1,pre_dispatch='1*n_jobs',scoring = scoring,refit = True,cv=fold)\n",
    "    search.fit(X_train,y_train)\n",
    "    best_score =search.score(X_test,y_test)\n",
    "    model_name = model.__class__.__name__\n",
    "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\n",
    "\n",
    "def bestModel(modelsAndParams):\n",
    "    modelPerformance = pd.DataFrame()\n",
    "    for item in modelsAndParams:\n",
    "        result = modelBestFit(item)\n",
    "        \n",
    "        modelPerformance = modelPerformance.append(result,ignore_index=True)\n",
    "        \n",
    "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\n",
    "    return modelPerformance\n",
    " \n",
    "result = bestModel(modelsWithParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4UPwf9YxrR5"
   },
   "outputs": [],
   "source": [
    "#bayesian search cv\n",
    "def modelBestFit(item):\n",
    "    model = item['model']\n",
    "    paramGrid = item['param']\n",
    "    search = RandomizedSearchCV(model, paramGrid,n_jobs=-1,pre_dispatch='1*n_jobs',scoring = scoring,refit = True,cv=fold)\n",
    "    search.fit(X_train,y_train)\n",
    "    best_score =search.score(X_test,y_test)\n",
    "    model_name = model.__class__.__name__\n",
    "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\n",
    "\n",
    "def bestModel(modelsAndParams):\n",
    "    modelPerformance = pd.DataFrame()\n",
    "    for item in modelsAndParams:\n",
    "        result = modelBestFit(item)\n",
    "        \n",
    "        modelPerformance = modelPerformance.append(result,ignore_index=True)\n",
    "        \n",
    "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\n",
    "    return modelPerformance\n",
    " \n",
    "result = bestModel(modelsWithParam)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MachineLearningV2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
