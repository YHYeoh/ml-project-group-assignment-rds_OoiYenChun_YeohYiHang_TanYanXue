{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['Basic', 'Graduation', 'Master', '2n Cycle', 'PhD'], dtype=object)]\n",
      "0    1906\n",
      "1     334\n",
      "Name: Response, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "DATASET_URL = \"https://gist.githubusercontent.com/YHYeoh/ad1a7f7170c72d621d05a70637540152/raw/5a6059c199e2c46d2f3d258f03d93cfea98e2749/marketing_campaign.csv\"\n",
    "data = pd.read_csv(DATASET_URL, sep = ';')\n",
    "\n",
    "pd.set_option('plotting.backend','pandas_bokeh')\n",
    "\n",
    "# data.fillna(method = \"ffill\", inplace = True)\n",
    "data.isnull().values.any()\n",
    "\n",
    "\n",
    "education_order = [['Basic', 'Graduation', 'Master', '2n Cycle', 'PhD']]\n",
    "ordinal_encoder = OrdinalEncoder(categories=education_order)\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "\n",
    "imr = IterativeImputer(random_state=42, max_iter=100, min_value=data['Income'].min())\n",
    "imr = imr.fit(data[['Income']])\n",
    "data['Income'] = imr.transform(data[['Income']]).ravel()\n",
    "\n",
    "\n",
    "data[\"Education\"] = (ordinal_encoder.fit_transform(data[\"Education\"].values.reshape(-1, 1))).astype(int)\n",
    "print(ordinal_encoder.categories_)\n",
    "\n",
    "data_copy = data.copy()\n",
    "marital_status_ohe = pd.get_dummies(data[\"Marital_Status\"],prefix=\"Marital\")\n",
    "ohe_cols = marital_status_ohe.columns\n",
    "data = pd.concat([data, marital_status_ohe], axis=1)\n",
    "\n",
    "data.drop([\"ID\", 'Dt_Customer',\"Z_CostContact\",\"Z_Revenue\",\"Marital_Status\"], axis=1, inplace=True)\n",
    "\n",
    "categorical = ['Marital_Status']\n",
    "numerical = ['Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n",
    "       'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n",
    "       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\n",
    "       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',\n",
    "       'Complain']\n",
    "numerical_no_bool = ['Education','Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
    "\n",
    "y = data.Response\n",
    "print(y.value_counts())\n",
    "X = data.drop(['Response'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.3)\n",
    "\n",
    "X_train_cont = X_train.drop(['AcceptedCmp3',\n",
    "       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2','Education','Complain','Year_Birth','Marital_Absurd', 'Marital_Alone', 'Marital_Divorced',\n",
    "       'Marital_Married', 'Marital_Single', 'Marital_Together',\n",
    "       'Marital_Widow', 'Marital_YOLO'],axis=1)\n",
    "\n",
    "X_test_cont = X_test.drop(['AcceptedCmp3',\n",
    "       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2','Education','Complain','Year_Birth','Marital_Absurd', 'Marital_Alone', 'Marital_Divorced',\n",
    "       'Marital_Married', 'Marital_Single', 'Marital_Together',\n",
    "       'Marital_Widow', 'Marital_YOLO'],axis=1)\n",
    "X_cont_column = X_train_cont.columns\n",
    "pcaX_train = X_train_cont\n",
    "pcaX_test = X_test_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getIterativeImputedIncome(data):\n",
    "\timr = IterativeImputer(random_state=42, max_iter=100, min_value= data['Income'].min())\n",
    "\timr = imr.fit(data[['Income']])\n",
    "\tdata['Income'] = imr.transform(data[['Income']]).ravel()\n",
    "\treturn data\n",
    "\n",
    "def getKNNImputedIncome(data):\n",
    "\timputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "\tdata = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\treturn data\n",
    "\n",
    "def getAgeFromDateBirth(data):\n",
    "\tyear = datetime.datetime.now().year\n",
    "\tdata['Year_Birth'] = data['Year_Birth'].apply(lambda x : year - x )\n",
    "\treturn data\n",
    "\n",
    "def extractFromDate(data):\n",
    "\tdata['enroll_year'] = pd.DatetimeIndex(data.Dt_Customer).year\n",
    "\tdata['enroll_month'] = pd.DatetimeIndex(data.Dt_Customer).month\n",
    "\tdata['enroll_day'] = pd.DatetimeIndex(data.Dt_Customer).day\n",
    "\tdata.drop(['Dt_Customer'], axis = 1, inplace= True)\n",
    "\treturn data\n",
    "\n",
    "def convertToDays(data):\n",
    "\tvfunc = np.vectorize(lambda x: (datetime.datetime.now() - x).days)\n",
    "\tdata['Dt_Customer'] = vfunc(pd.DatetimeIndex(data.Dt_Customer).to_pydatetime())\n",
    "\treturn data\n",
    "\n",
    "def getBinnedIncome(data):\n",
    "\tdata['Income'] = (pd.cut(data['Income'], bins=[0, 15000, 60000, 110000, 300000], labels=False, precision=0)).convert_dtypes()\n",
    "\treturn data\n",
    "\n",
    "def getNormalizedIncome(data):\n",
    "\tdata = data[(np.abs(stats.zscore(data[['Income']])) < 3)]\n",
    "\treturn data\n",
    "\n",
    "def getNormalizedAndBinnedIncome(data):\n",
    "\tdata = data[(np.abs(stats.zscore(data[['Income']])) < 3)]\n",
    "\tdata['Income'] = pd.cut(data['Income'], bins=[0, 15000, 60000, 110000, 300000], labels=False, precision=0).convert_dtypes()\n",
    "\treturn data\n",
    "\n",
    "def getHypeYearBirth(data):\n",
    "\thyper_year_birth = [\n",
    "\t\tdata,\n",
    "\t\tgetAgeFromDateBirth(data.copy()),\n",
    "\t]\n",
    "\treturn hyper_year_birth\n",
    "\n",
    "\n",
    "\n",
    "def getPreprocessingIncome(data):\n",
    "\tpreprocessing_income = [\n",
    "\t\tgetIterativeImputedIncome(data.copy()),\n",
    "\t\tgetKNNImputedIncome(data.copy()),\n",
    "\t\tdata.copy().fillna(method = \"ffill\"), #ffill\n",
    "\t\tdata.copy().fillna(method = \"bfill\"), #bfill\n",
    "\t\tdata.copy().fillna(data.mean()), #mean imputed\n",
    "\t\tdata.copy().fillna(data.median()), #median imputed\n",
    "\t\tdata.copy().dropna(subset=['Income'])\n",
    "\t]\n",
    "\treturn preprocessing_income\n",
    "\n",
    "def getHypeIncome(data):\n",
    "\thyper_income = [\n",
    "\tdata,\n",
    "\tgetBinnedIncome(data.copy()),\n",
    "\tgetNormalizedIncome(data.copy()),\n",
    "\tgetNormalizedAndBinnedIncome(data.copy())\n",
    "\t]\n",
    "\treturn hyper_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SalaryRange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasmethod(obj, name):\n",
    "\treturn inspect.ismethod(getattr(obj, name, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setupPreprocessPipeline(scaler):\n",
    "\tss = Pipeline(steps=[('scaler',scaler)])\n",
    "\t#ohe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "\tpreprocess = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('cont', ss, numerical_no_bool)\n",
    "                        #('cat', ohe, categorical),\n",
    "                        #('le', le, ordinal),\n",
    "                        ],remainder='passthrough')\n",
    "\treturn preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(classifier, feature_names, scaler_name):\n",
    "\tif (hasattr(classifier,'coef_')):\n",
    "\t\timportance = classifier.coef_[0]\n",
    "\telif (hasattr(classifier,'coefs_')):\n",
    "\t\timportance = classifier.coefs_\n",
    "\telif (hasattr(classifier,'feature_importances_')):\n",
    "\t\timportance = classifier.feature_importances_\n",
    "\telse:\n",
    "\t\tprint(\"Cannot extract feature importance, skipping\")\n",
    "\t\treturn\n",
    "\n",
    "\t#for i,v in enumerate(importance):\n",
    "\t#\tprint('Feature: %d, Score: %.5f' % (i,v))\n",
    "\tzipped = zip(feature_names, importance)\n",
    "\tdf = pd.DataFrame(zipped, columns=[\"feature\", \"value\"])\n",
    "\t# Sort the features by the absolute value of their coefficient\n",
    "\tdf[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\n",
    "\tdf[\"colors\"] = df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\n",
    "\tdf = df.sort_values(\"abs_value\", ascending=False)\n",
    "\t# plot feature importance\n",
    "\tfig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "\tsns.barplot(x=\"feature\",\n",
    "\t            y=\"value\",\n",
    "\t            data=df.head(20),\n",
    "\t           palette=df.head(20)[\"colors\"])\n",
    "\tplt.gcf().subplots_adjust(bottom=0.30)\n",
    "\tax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=14)\n",
    "\tax.set_title(\"Top 20 Features for {} w/ {}\".format(classifier.__class__.__name__, scaler_name), fontsize=25)\n",
    "\tax.set_ylabel(\"Coef\", fontsize=22)\n",
    "\tax.set_xlabel(\"Feature Name\", fontsize=22)\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def overallClassificationReport(model,classes):\n",
    "\tvisualizer = ClassificationReport(model, classes=classes, support=True) #might can change\n",
    "\tvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "\tif(model.__class__.__name__ == \"XGBClassifier\"): #special treatment for xgboost as it reordered column\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\treorderedColumn = model.get_booster().feature_names\n",
    "\t\treordered_Xtest = X_test[reorderedColumn] #reorderColumn\n",
    "\t\tvisualizer.score(X_test, y_test)        \n",
    "\t\tvisualizer.show()\n",
    "\t\treturn\n",
    "\tvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "\tvisualizer.show()\n",
    "\n",
    "\n",
    "def overall_feature_importance(model,X_train,y_train):\n",
    "\tlabels = list(map(lambda s: s.title(), X.columns))\n",
    "\tviz = FeatureImportances(model, labels=labels,encoder={1: 'yes',0: 'no'}, relative=False, topn = 8)\n",
    "\tviz.fit(X, y)\n",
    "\tviz.show()\n",
    "\n",
    "def has_feature_imp(classifier):\n",
    "\tstatus = False\n",
    "\tif (hasattr(classifier,'coef_')):\n",
    "\t\tstatus = True\n",
    "\telif (hasattr(classifier,'coefs_')):\n",
    "\t\tstatus = True\n",
    "\telif (hasattr(classifier,'feature_importances_')):\n",
    "\t\tstatus = True\n",
    "\tprint(\"Cannot extract feature importance, skipping\")\n",
    "\treturn status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def pcaComparison(scaler,X_train,y_train,X_test,y_test):\n",
    "#     cv=10\n",
    "#     scaler = scaler\n",
    "#     X_train_scaled = scaler.fit_transform(pcaX_train,y_train)\n",
    "#     X_test_scaled = scaler.transform(pcaX_test)\n",
    "#     pcaPerformance = pd.DataFrame()\n",
    "#     for transformer in pcaParamGrid :\n",
    "#         pca = transformer['transformer']\n",
    "#         param_grid =  transformer['param_grid']\n",
    "#         PCASearch = RandomizedSearchCV(pca,param_grid,n_iter=10,verbose=2,\n",
    "#                          scoring= scorer,\n",
    "#                          n_jobs=-1,cv=cv,random_state=42)\n",
    "#         PCASearch = PCASearch.fit(X_train_scaled,y_train)\n",
    "\n",
    "#         param = PCASearch.best_params_\n",
    "#         name = PCASearch.best_estimator_.__class__.__name__\n",
    "#         score = PCASearch.best_score_\n",
    "#         pcaResult = {\"Model\":name,\"MSE\": score,\"Parameter\":param,\"scaler\":scaler}\n",
    "#         pcaPerformance = pcaPerformance.append(pcaResult,ignore_index=True)\n",
    "#     pcaPerformance.sort_values(by='MSE',ascending=True,inplace=True)\n",
    "#     return pcaPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcaParamGrid = [{\n",
    "#     'transformer':PCA(),'param_grid':{'n_components':list(range(2,8)),'random_state':[42]\n",
    "#                                       ,'whiten':[True,False]\n",
    "#                                       ,'svd_solver':['full', 'arpack', 'randomized']}\n",
    "# },{\n",
    "#     'transformer':KernelPCA(),\n",
    "#     'param_grid':{'gamma':loguniform(1e-6, 100),'n_components':list(range(2,8))\n",
    "#                   ,'random_state':[42],'kernel':['rbf'],\n",
    "#                   'alpha':loguniform(1e-6, 100),'eigen_solver':['dense', 'arpack']\n",
    "#                   ,'n_jobs':[-1],'max_iter':list(range(1,1000)),'fit_inverse_transform':[True,False]}\n",
    "# },\n",
    "#     {\n",
    "#         'transformer':KernelPCA(),\n",
    "#     'param_grid':{'gamma':loguniform(1e-6, 100),'n_components':list(range(2,8))\n",
    "#                   ,'random_state':[42],'kernel':['poly'],\n",
    "#                   'alpha':loguniform(1e-6, 100),'degree':list(range(3,8))\n",
    "#                   ,'n_jobs':[-1],'max_iter':list(range(1,1000)),'eigen_solver':['dense', 'arpack'],\n",
    "#                  'fit_inverse_transform':[True,False]}\n",
    "#     }\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentScalerResult = pd.DataFrame()\n",
    "# scalers = [StandardScaler(),MinMaxScaler(),MaxAbsScaler(), RobustScaler(),QuantileTransformer()]\n",
    "\n",
    "# for scaler in scalers:\n",
    "#     pcaResult = pcaComparison(scaler,pcaX_train,y_train,pcaX_test,y_test)\n",
    "#     differentScalerResult = differentScalerResult.append(pcaResult,ignore_index=True)\n",
    "# differentScalerResult.sort_values(by='MSE',ascending=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentScalerResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use the same scaler same as pca\n",
    "# scaler = MaxAbsScaler()\n",
    "# print(scaler)\n",
    "# scaledXTrain = pd.DataFrame(scaler.fit_transform(pcaX_train),columns=X_cont_column)\n",
    "# scaledXTest = pd.DataFrame(scaler.transform(pcaX_test),columns=X_cont_column)\n",
    "\n",
    "# km_list = list()\n",
    "# for clust in range(2,21):\n",
    "#     km = KMeans(n_clusters=clust, random_state=42)\n",
    "#     km = km.fit(scaledXTrain)\n",
    "#     y_pred = km.predict(scaledXTrain)\n",
    "#     sh = silhouette_score(scaledXTrain,y_pred)\n",
    "#     km_list.append(pd.Series({'clusters': clust, \n",
    "#                               'inertia': km.inertia_,\n",
    "#                               'model': km,\n",
    "#                              'sil':sh}))\n",
    "# io.reset_output()\n",
    "# io.output_notebook()\n",
    "\n",
    "\n",
    "# plot_data = (pd.concat(km_list, axis=1)\n",
    "#              .T\n",
    "#              [['clusters','inertia','sil']]\n",
    "#              .set_index('clusters'))\n",
    "# plot_dataInertia=plot_data['inertia'].astype(float)\n",
    "# ax = plot_dataInertia.plot(marker='o')\n",
    "\n",
    "# #concat so show sihhoutte score\n",
    "# plot_dataSil=plot_data['sil'].astype(float)\n",
    "# ax = plot_dataSil.plot(marker='o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saw 6 cluster has a good silhouette score, create kmean column with 6 cluster\n",
    "# km = KMeans(n_clusters=6, random_state=42)\n",
    "# km = km.fit(scaledXTrain)\n",
    "# kmeanTrainColumn = pd.Series(km.predict(scaledXTrain), name=\"KMeansTrain\")\n",
    "# kmeanTestColumn = pd.Series(km.predict(scaledXTest), name=\"KMeansTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeanTrainColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=1\n",
    "# for rowNo,result in differentScalerResult.iterrows():\n",
    "#     mse,model,param,scaler = result\n",
    "#     print(str(i)+\" \"+str(model))\n",
    "#     print(mse)\n",
    "#     print(param)\n",
    "#     print(scaler)\n",
    "#     print(\"\\n\")\n",
    "#     if(i==5):\n",
    "#         break\n",
    "#     i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #KernelPCA has the lowest error\n",
    "# kpca = KernelPCA(alpha=7.577453045410569, degree= 7, eigen_solver= \"dense\"\n",
    "#                  , fit_inverse_transform= True, gamma= 1.2767906371238508e-06\n",
    "#                  , kernel= \"poly\", max_iter= 601, n_components= 5, n_jobs= -1, random_state= 42)\n",
    "# scaler= MaxAbsScaler()\n",
    "# X_train_scaled = scaler.fit_transform(pcaX_train,y_train)\n",
    "# X_test_scaled = scaler.transform(pcaX_test)\n",
    "\n",
    "# kpca = kpca.fit(X_train_scaled,y_train)\n",
    "# transformedXTrain = kpca.transform(X_train_scaled)\n",
    "# transformedXTest=kpca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_variance = np.var(transformedXTrain, axis=0)\n",
    "# explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "# print(\"Explained Variance : \\n\" +str(explained_variance))\n",
    "# print(\"Explained Variance Ratio: \\n\"+str(explained_variance_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kpcaCol = [\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\"]\n",
    "# kpcaTrain = pd.DataFrame(transformedXTrain,columns = kpcaCol)\n",
    "# kpcaTest = pd.DataFrame(transformedXTest, columns = kpcaCol)\n",
    "# #reset all train test columns to ensure consistency before merging\n",
    "# X_train = X_train.reset_index()\n",
    "# X_test = X_test.reset_index()\n",
    "# y_train = y_train.reset_index()\n",
    "# y_test = y_test.reset_index()\n",
    "# #drop all continuous columns\n",
    "\n",
    "# X_train_Kmeans = X_train.copy()\n",
    "# X_test_Kmeans = X_test.copy()\n",
    "# X_train_PCA_Kmeans = X_train.drop(columns=X_cont_column,axis=1)\n",
    "# X_test_PCA_Kmeans = X_test.drop(columns=X_cont_column,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove default unimportant column\n",
    "unimportant = [\"Year_Birth\"]\n",
    "# X_train_Kmeans = X_train_Kmeans.drop(columns=unimportant,axis=1)\n",
    "# X_test_Kmeans = X_test_Kmeans.drop(columns=unimportant,axis=1)\n",
    "# X_train_PCA_Kmeans = X_train_PCA_Kmeans.drop(columns=unimportant,axis=1)\n",
    "# X_test_PCA_Kmeans = X_test_PCA_Kmeans.drop(columns=unimportant,axis=1)\n",
    "X_train = X_train.drop(columns=unimportant,axis=1)\n",
    "X_test = X_test.drop(columns=unimportant,axis=1)\n",
    "y_train = y_train.drop(columns=['index'],axis=1)\n",
    "y_test = y_test.drop(columns=['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994     0\n",
       "2162    0\n",
       "906     1\n",
       "572     0\n",
       "1877    0\n",
       "       ..\n",
       "1638    0\n",
       "1095    0\n",
       "1130    0\n",
       "1294    0\n",
       "860     0\n",
       "Name: Response, Length: 1568, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add kmean column into all known data except x_train\n",
    "# X_train_Kmeans[\"KMean\"]=kmeanTrainColumn\n",
    "# X_test_Kmeans[\"KMean\"]=kmeanTestColumn\n",
    "# X_train_PCA_Kmeans[\"KMean\"]=kmeanTrainColumn\n",
    "# X_test_PCA_Kmeans[\"KMean\"]=kmeanTestColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_PCA_Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #addPCA column\n",
    "# kpcaCol = [\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\"]\n",
    "\n",
    "# #PCA and Kmeans\n",
    "# X_train_PCA_Kmeans[kpcaCol]=kpcaTrain\n",
    "# X_test_PCA_Kmeans[kpcaCol]=kpcaTest\n",
    "\n",
    "# #PCA only\n",
    "# X_train_PCA = kpcaTrain\n",
    "# X_test_PCA = kpcaTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf_sampler = RBFSampler(gamma = 5.0,n_components=100)\n",
    "# X_train_rbf = rbf_sampler.fit_transform(X_train_PCA)\n",
    "# X_test_rbf = rbf_sampler.transform(X_test_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_trainS, y_trainS = sampler.fit_resample(X_train, y_train.copy())\n",
    "# # stratified(SMOTE) x with k means\n",
    "# X_train_Kmeans, y_trainR = sampler.fit_resample(X_train_Kmeans, y_train.copy())\n",
    "# # stratified(SMOTE) x with pca\n",
    "# X_train_PCA, y_trainR = sampler.fit_resample(X_train_PCA, y_train.copy())\n",
    "# # stratified(SMOTE) x with pca and k means\n",
    "# X_train_PCA_Kmeans, y_trainR = sampler.fit_resample(X_train_PCA_Kmeans, y_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomOverSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-9d31d07369d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#X_train_Kmeans, y_train.copy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0moversampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mundersampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'majority'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mX_trainOV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_trainOV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moversampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomOverSampler' is not defined"
     ]
    }
   ],
   "source": [
    "# sampler = SMOTE()\n",
    "# stratified(SMOTE) x\n",
    "# stratified(SMOTE) x with k means\n",
    "# stratified(SMOTE) x with pca\n",
    "# stratified(SMOTE) x with pca and k means\n",
    "\n",
    "#X_train, y_train.copy()\n",
    "#X_train_Kmeans, y_train.copy()\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=0.5)\n",
    "undersampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_trainOV,y_trainOV = oversampler.fit_resample(X_train, y_train)\n",
    "X_trainUN, trainUN = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = [\n",
    "    {\n",
    "        'DataName':'Unmodified Dataset','X_train':X_train,'y_train':y_train\n",
    "    },\n",
    "    {\n",
    "        'DataName':'Oversampled Dataset','X_train':X_trainOV,'y_train':y_trainOV\n",
    "    },\n",
    "    {\n",
    "        'DataName':'Undersampled Dataset','X_train':X_trainOV,'y_train':y_trainOV\n",
    "    }\n",
    "]\n",
    "model_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(classifier, cv,datasetName,X_train,y_train):\n",
    "\tscalers = [StandardScaler(),MinMaxScaler(),MaxAbsScaler(), RobustScaler(),QuantileTransformer()]\n",
    "\ttrain_acc = []\n",
    "\ttest_acc = []\n",
    "\tmean = []\n",
    "\tresult = []\n",
    "\tfor scaler in scalers:\n",
    "\t\tfpr = None\n",
    "\t\ttpr = None\n",
    "\t\tpreprocess = setupPreprocessPipeline(scaler)\n",
    "\t\tpipeline = Pipeline(steps=[\n",
    "\t        ('preprocess', preprocess),\n",
    "\t        ('classifier', classifier)\n",
    "\t\t])\n",
    "\n",
    "\t\ttrain_acc = []\n",
    "\t\ttest_acc = []\n",
    "\t\ttrain_recall = []\n",
    "\t\ttest_recall = []\n",
    "\t\ttrain_precision = []\n",
    "\t\ttest_precision = []\n",
    "\t\ttrain_f1 = []\n",
    "\t\ttest_f1 = []\n",
    "\t\tmean = []\n",
    "\t\t\n",
    "\t\tfor train_ind, val_ind in cv.split(X_train, y_train):\n",
    "\t\t\tX_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind]\n",
    "\t\t\tpipeline.fit(X_t, y_t)\n",
    "\t\t\ty_hat_t = pipeline.predict(X_t)\n",
    "\t\t\ttrain_acc.append(accuracy_score(y_t, y_hat_t))\n",
    "\t\t\ttrain_recall.append(recall_score(y_t, y_hat_t))\n",
    "\t\t\ttrain_precision.append(precision_score(y_t, y_hat_t))\n",
    "\t\t\ttrain_f1.append(f1_score(y_t, y_hat_t))\n",
    "\t\t\tX_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind] \n",
    "\t\t\ty_hat_val = pipeline.predict(X_val)\n",
    "\t\t\ttest_acc.append(accuracy_score(y_val, y_hat_val))\n",
    "\t\t\ttest_recall.append(recall_score(y_val, y_hat_val))\n",
    "\t\t\ttest_precision.append(precision_score(y_val, y_hat_val))\n",
    "\t\t\ttest_f1.append(f1_score(y_val, y_hat_val))\n",
    "\n",
    "\t\t#ohe_cols = list(pipeline.named_steps['preprocess'].named_transformers_['cat'].named_steps['ohe'].get_feature_names(input_features=categorical))\n",
    "\t\tnumeric_feature_list = list(numerical)\n",
    "\t\tfor i in ohe_cols:\n",
    "\t\t\tnumeric_feature_list.append(i)\n",
    "\t\t#print(len(numeric_feature_list))\n",
    "\t\t#evaluation(y_val, y_hat_val, 'Confusion Matrix {} + {}'.format(classifier.__class__.__name__, scaler.__class__.__name__).strip())\n",
    "\t\tprint('Mean Training Accuracy: {} | Standard Deviation: {}'.format(np.mean(train_acc),np.std(test_acc)))\n",
    "\t\tprint('Mean Validation Accuracy: {} | Standard Deviation: {}'.format(np.mean(test_acc),np.std(test_acc)))\n",
    "\t\tprint('\\n')\n",
    "# \t\tfeature_importance(classifier, numeric_feature_list, scaler.__class__.__name__ )\n",
    "\t\tprint(y_val.shape,y_hat_val.shape)\n",
    "\t\t# \n",
    "# \t\tif hasmethod(pipeline['classifier'], 'predict_proba'):\n",
    "# \t\t\tfpr,tpr = ROC_Curve_Plot(pipeline,X_val,y_val,classifier.__class__.__name__ +\" w \"+scaler.__class__.__name__)\n",
    "\t\tmodel_result.append({\n",
    "            'classifier':classifier.__class__.__name__,\n",
    "            'scalerName':scaler.__class__.__name__,\n",
    "            'datasetName':datasetName,\n",
    "            'train_accuracy':np.mean(train_acc),\n",
    "            'test_accuracy':np.mean(test_acc),\n",
    "            'train_recall':np.mean(train_recall),\n",
    "            'test_recall':np.mean(test_recall),\n",
    "            'train_precision':np.mean(train_precision),\n",
    "            'test_precision':np.mean(test_precision),\n",
    "            'train_f1':np.mean(train_f1),\n",
    "            'test_f1':np.mean(test_f1)\n",
    "        })\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# models = [\n",
    "# \t#Lasso(),\n",
    "# # \tSGDClassifier(max_iter = 1000, tol=1e-3,penalty = \"elasticnet\"),\n",
    "# # \tLinearSVC(), \n",
    "# # \tGaussianProcessClassifier(),\n",
    "# # \tExtraTreesClassifier(), \n",
    "# # \tBernoulliNB(),\n",
    "# \tLogisticRegressionCV(max_iter= 1200), \n",
    "# # \tRidgeClassifierCV(),\n",
    "# \tSVC(kernel = 'linear',max_iter= -1), \n",
    "# # \tPerceptron(),\n",
    "# # \tPassiveAggressiveClassifier(), \n",
    "# # \tDecisionTreeClassifier(), #no coef \n",
    "# # \tKNeighborsClassifier(),#no feat_import, use permutation_importance \n",
    "# # \tGaussianNB(), #no feat_import, use permutation_importance \n",
    "# \tLGBMClassifier(),#no coef \n",
    "# # \tRandomForestClassifier(), #no coef \n",
    "# # \tGradientBoostingClassifier(),#no coef \n",
    "# # \tPassiveAggressiveClassifier(), \n",
    "# # \tExtraTreesClassifier(), #no coef \n",
    "# \tXGBClassifier(),\n",
    "# \tAdaBoostClassifier(), #no coef\n",
    "# # \tMLPClassifier() #mlp not working\n",
    "# \t]\n",
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "\t#Lasso(),\n",
    "# \tSGDClassifier(max_iter = 1000, tol=1e-3,penalty = \"elasticnet\"),\n",
    "# \tLinearSVC(), \n",
    "# \tGaussianProcessClassifier(),\n",
    "# \tExtraTreesClassifier(), \n",
    "# # \tBernoulliNB(),\n",
    "# \tLogisticRegressionCV(max_iter= 1200), \n",
    "# # \tRidgeClassifierCV(),\n",
    "# \tSVC(kernel = 'linear',max_iter= -1), \n",
    "# \tPerceptron(),\n",
    "# \tPassiveAggressiveClassifier(), \n",
    "# \tDecisionTreeClassifier(), #no coef \n",
    "# \tKNeighborsClassifier(),#no feat_import, use permutation_importance \n",
    "# \tGaussianNB(), #no feat_import, use permutation_importance \n",
    "# \tLGBMClassifier(),#no coef \n",
    "\tRandomForestClassifier(), #no coef \n",
    "# \tGradientBoostingClassifier(),#no coef \n",
    "# \tPassiveAggressiveClassifier(), \n",
    "# \tExtraTreesClassifier(), #no coef \n",
    "# \tXGBClassifier(),\n",
    "# \tAdaBoostClassifier(), #no coef\n",
    "# \tMLPClassifier() #mlp not working\n",
    "\t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_result= []\n",
    "classes = [\"no\", \"yes\"]\n",
    "\n",
    "for data in dataList:\n",
    "    print(data['DataName'])\n",
    "    xTrainData = data['X_train']\n",
    "    yTrainData = data['y_train']\n",
    "    for model in models:\n",
    "        print(model.__class__.__name__)\n",
    "        model_result.append(cross_validate(model,StratifiedKFold(),data['DataName'],xTrainData, yTrainData))\n",
    "        overallClassificationReport(model,classes)\n",
    "        if hasmethod(model, 'predict_proba'):\n",
    "            ROC_Curve_Plot(model,X_test,y_test,\"Overall \"+model.__class__.__name__)\n",
    "        if has_feature_imp(model) :\n",
    "            overall_feature_importance(model,xTrainData, yTrainData)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = pd.DataFrame.from_records(model_result,columns=['classifier','scalerName','datasetName','train_accuracy','test_accuracy','train_recall','test_recall','train_precision','test_precision','train_f1','test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultDF.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_URL = \"https://gist.githubusercontent.com/YHYeoh/ad1a7f7170c72d621d05a70637540152/raw/5a6059c199e2c46d2f3d258f03d93cfea98e2749/marketing_campaign.csv\"\n",
    "# data = pd.read_csv(DATASET_URL, sep = ';')\n",
    "\n",
    "# pd.set_option('plotting.backend','pandas_bokeh')\n",
    "\n",
    "# # data.fillna(method = \"ffill\", inplace = True)\n",
    "# data.isnull().values.any()\n",
    "\n",
    "\n",
    "# education_order = [['Basic', 'Graduation', 'Master', '2n Cycle', 'PhD']]\n",
    "# ordinal_encoder = OrdinalEncoder(categories=education_order)\n",
    "# enc = OneHotEncoder()\n",
    "\n",
    "\n",
    "# imr = IterativeImputer(random_state=42, max_iter=100, min_value=data['Income'].min())\n",
    "# imr = imr.fit(data[['Income']])\n",
    "# data['Income'] = imr.transform(data[['Income']]).ravel()\n",
    "\n",
    "\n",
    "# data[\"Education\"] = (ordinal_encoder.fit_transform(data[\"Education\"].values.reshape(-1, 1))).astype(int)\n",
    "# print(ordinal_encoder.categories_)\n",
    "\n",
    "# data_copy = data.copy()\n",
    "# marital_status_ohe = pd.get_dummies(data[\"Marital_Status\"],prefix=\"Marital\")\n",
    "# ohe_cols = marital_status_ohe.columns\n",
    "# data = pd.concat([data, marital_status_ohe], axis=1)\n",
    "\n",
    "# data.drop([\"ID\", 'Dt_Customer',\"Z_CostContact\",\"Z_Revenue\",\"Marital_Status\"], axis=1, inplace=True)\n",
    "\n",
    "# categorical = ['Marital_Status']\n",
    "# numerical = ['Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n",
    "#        'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "#        'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "#        'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n",
    "#        'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\n",
    "#        'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',\n",
    "#        'Complain']\n",
    "# numerical_no_bool = ['Education','Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
