{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV \n",
    "from sklearn.linear_model import Perceptron, LogisticRegressionCV, RidgeClassifierCV, SGDClassifier, PassiveAggressiveClassifier, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score,mean_absolute_error, confusion_matrix, silhouette_score\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc, classification_report,precision_score,recall_score,log_loss,f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, LabelEncoder, OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer,minmax_scale,PolynomialFeatures\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn import tree\n",
    "import pandas_bokeh\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from numpy import mean, std\n",
    "import pandas.testing as tm\n",
    "from scipy import stats\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "\n",
    "from yellowbrick.features import PCA as PCA_YB\n",
    "from yellowbrick.features.radviz import RadViz\n",
    "from yellowbrick.features import pca_decomposition\n",
    "from yellowbrick.features import Manifold\n",
    "from yellowbrick.features import JointPlotVisualizer\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "from yellowbrick.features import ParallelCoordinates\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "\n",
    "#other\n",
    "from math import sqrt\n",
    "import inspect\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.stats import loguniform, uniform\n",
    "from bokeh import io\n",
    "import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import eli5\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATASET_URL = \"https://gist.githubusercontent.com/YHYeoh/ad1a7f7170c72d621d05a70637540152/raw/5a6059c199e2c46d2f3d258f03d93cfea98e2749/marketing_campaign.csv\"\n",
    "data = pd.read_csv(DATASET_URL, sep = ';')\n",
    "\n",
    "education_order = [['Basic', 'Graduation', 'Master', '2n Cycle', 'PhD']]\n",
    "ordinal_encoder = OrdinalEncoder(categories=education_order)\n",
    "\n",
    "data[\"Education\"] = (ordinal_encoder.fit_transform(data[\"Education\"].values.reshape(-1, 1))).astype(int)\n",
    "# print(ordinal_encoder.categories_)\n",
    "\n",
    "#encode categorical column\n",
    "categorical = ['Marital_Status']\n",
    "marital_status_ohe = pd.get_dummies(data.Marital_Status,prefix=\"Marital\")\n",
    "data = data.join(marital_status_ohe)\n",
    "\n",
    "#drop original column after encoding\n",
    "data.drop(['Marital_Status'], axis = 1,inplace = True)\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "tunable_cols = [\"Year_Birth\", \"Income\",\"Dt_Customer\"]\n",
    "\n",
    "\n",
    "y = data.Response\n",
    "X = data.drop(\"Response\",axis=1)\n",
    "\n",
    "numerical_bool_col = [x for x in data.columns if data[x].isin([0,1]).all()] # print(numerical_bool_col)\n",
    "numerical_scalable_col = [x for x in data.columns if x not in numerical_bool_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# import numba\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "\n",
    "def getKeyVal(inDict):\n",
    "  for i in inDict:\n",
    "    key=i\n",
    "    val= inDict[i]\n",
    "  return key,val\n",
    "\n",
    "def getIterativeImputedIncome(data):\n",
    "  imr = IterativeImputer(random_state=42, max_iter=100, min_value= data['Income'].min())\n",
    "  imr = imr.fit(data[['Income']])\n",
    "  data['Income'] = imr.transform(data[['Income']]).ravel()\n",
    "  return data\n",
    "\n",
    "def getKNNImputedIncome(data):\n",
    "\timputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "\tdata = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\treturn data\n",
    "\n",
    "def getAgeFromDateBirth(data):\n",
    "\tyear = datetime.datetime.now().year\n",
    "\tdata['Year_Birth'] = data['Year_Birth'].apply(lambda x : year - x )\n",
    "\treturn data\n",
    "\n",
    "def extractFromDate(data):\n",
    "\tdata['enroll_year'] = pd.DatetimeIndex(data.Dt_Customer).year\n",
    "\tdata['enroll_month'] = pd.DatetimeIndex(data.Dt_Customer).month\n",
    "\tdata['enroll_day'] = pd.DatetimeIndex(data.Dt_Customer).day\n",
    "\tdata.drop(['Dt_Customer'], axis = 1, inplace= True)\n",
    "\treturn data\n",
    "\n",
    "def convertToDays(data):\n",
    "\tvfunc = np.vectorize(lambda x: (datetime.datetime.now() - x).days)\n",
    "\tdata['Dt_Customer'] = vfunc(pd.DatetimeIndex(data.Dt_Customer).to_pydatetime())\n",
    "\treturn data\n",
    "\n",
    "def getBinnedIncome(data):\n",
    "\tdata['Income'] = pd.cut(data['Income'], bins=[0, 15000, 60000, 110000, 700000], labels=False, precision=0).convert_dtypes()\n",
    "\treturn data\n",
    "\n",
    "def getNormalizedIncome(data):\n",
    "    \n",
    "\tdata = data[(np.abs(stats.zscore(data[['Income']])) < 3)]\n",
    "\treturn data\n",
    "\n",
    "def getNormalizedAndBinnedIncome(data):\n",
    "\tdata = data[(np.abs(stats.zscore(data[['Income']])) < 3)]\n",
    "\tdata['Income'] = pd.cut(data['Income'], bins=[0, 15000, 60000, 110000, 700000], labels=False, precision=0).convert_dtypes()\n",
    "\treturn data\n",
    "\n",
    "\n",
    "def getOverSamplingData(x,y):\n",
    "    oversampler = RandomOverSampler(sampling_strategy=0.5)\n",
    "    x,y = oversampler.fit_resample(x, y)\n",
    "    return (x,y)\n",
    "\n",
    "\n",
    "def getUnderSamplingData(x,y):\n",
    "    undersampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "    x,y = undersampler.fit_resample(x, y)\n",
    "    return (x,y)\n",
    "\n",
    "def getHypeYearBirth(data):\n",
    "\thyper_year_birth = [\n",
    "    {'Not age engineering':data},\n",
    "\t\t{'Age':getAgeFromDateBirth(data.copy())}\n",
    "\t]\n",
    "\treturn hyper_year_birth\n",
    "\n",
    "hyper_dt_customer = [\n",
    "\t{'extractFromDate':extractFromDate(data.copy())},\n",
    "\t{'convertToDays':convertToDays(data.copy())}\n",
    "]\n",
    "\n",
    "def getPreprocessingIncome(data):\n",
    "\tpreprocessing_income = [\n",
    "    {'getIterativeImputedIncome':getIterativeImputedIncome(data.copy())},\n",
    "\t\t{'getKNNImputedIncome':getKNNImputedIncome(data.copy())},\n",
    "\t\t{'fillNa method = ffill':data.copy().fillna(method = \"ffill\")}, #ffill\n",
    "\t\t{'fillNa method = bfill':data.copy().fillna(method = \"bfill\")}, #bfill\n",
    "\t\t{'mean imputed':data.copy().fillna(data.mean())}, #mean imputed\n",
    "\t\t{'median imputed':data.copy().fillna(data.median())}, #median imputed\n",
    "\t\t{'Income dropped':data.copy().dropna(subset=['Income'])}\n",
    "\t]\n",
    "\treturn preprocessing_income\n",
    "\n",
    "def getHypeIncome(data):\n",
    "\thyper_income = [\n",
    "  {'No income engineering':data},\n",
    "\t{'Binned Income':getBinnedIncome(data.copy())},\n",
    "\t{'Normalized Income':getNormalizedIncome(data.copy())},\n",
    "\t{'Binned and Normalized Income':getNormalizedAndBinnedIncome(data.copy())}\n",
    "\t]\n",
    "\treturn hyper_income\n",
    "\n",
    "\n",
    "def getDataSampling(x,y):\n",
    "\thyper_oversampling = [\n",
    "  {'No data sampling':(x,y)},\n",
    "\t{'Under sampling':getUnderSamplingData(x,y)},\n",
    "\t{'Over sampling':getOverSamplingData(x,y)},\n",
    "\t]\n",
    "\treturn hyper_oversampling\n",
    "\n",
    "\n",
    "# hyperparams = {\n",
    "# \t\"Year_Birth\":[ data['Year_Birth'], getAgeFromDateBirth(data.copy())],\t\n",
    "# \t\"Income\": [data['Income'], getBinnedIncome(data['Income']), getNormalizedIncome()]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2240, 35), (2240,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasmethod(obj, name):\n",
    "\treturn inspect.ismethod(getattr(obj, name, None))\n",
    "\n",
    "def setupPreprocessPipeline(scaler,numerical_no_bool):\n",
    "\tss = Pipeline(steps=[('scaler',scaler)])\n",
    "\t#ohe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "\tpreprocess = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('cont', ss, numerical_no_bool)\n",
    "                        #('cat', ohe, categorical),\n",
    "                        #('le', le, ordinal),\n",
    "                        ],remainder='passthrough')\n",
    "\treturn preprocess\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def overallClassificationReport(model,classes):\n",
    "\tvisualizer = ClassificationReport(model, classes=classes, support=True) #might can change\n",
    "\tvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "\tif(model.__class__.__name__ == \"XGBClassifier\"): #special treatment for xgboost as it reordered column\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\treorderedColumn = model.get_booster().feature_names\n",
    "\t\treordered_Xtest = X_test[reorderedColumn] #reorderColumn\n",
    "\t\tvisualizer.score(X_test, y_test)        \n",
    "\t\tvisualizer.show()\n",
    "\t\treturn\n",
    "\tvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "\tvisualizer.show()\n",
    "\n",
    "\n",
    "def overall_feature_importance(model,X_train,y_train):\n",
    "\tlabels = list(map(lambda s: s.title(), X.columns))\n",
    "\tviz = FeatureImportances(model, labels=labels,encoder={1: 'yes',0: 'no'}, relative=False, topn = 8)\n",
    "\tviz.fit(X, y)\n",
    "\tviz.show()\n",
    "\n",
    "def has_feature_imp(classifier):\n",
    "\tstatus = False\n",
    "\tif (hasattr(classifier,'coef_')):\n",
    "\t\tstatus = True\n",
    "\telif (hasattr(classifier,'coefs_')):\n",
    "\t\tstatus = True\n",
    "\telif (hasattr(classifier,'feature_importances_')):\n",
    "\t\tstatus = True\n",
    "\tprint(\"Cannot extract feature importance, skipping\")\n",
    "\treturn status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(classifier, cv,X_train,y_train,dtColumnStatus\n",
    "                   ,incomePreproStatus,IncomeEngiStatus,yearProcessStatus,dataSamplingStatus,numerical_no_bool):\n",
    "\tscalers = [StandardScaler(),MinMaxScaler(),MaxAbsScaler(), RobustScaler(),QuantileTransformer()]\n",
    "\ttrain_acc = []\n",
    "\ttest_acc = []\n",
    "\tmean = []\n",
    "\tresult = []\n",
    "\tfor scaler in scalers:\n",
    "\t\tpreprocess = setupPreprocessPipeline(scaler,numerical_no_bool)\n",
    "\t\tpipeline = Pipeline(steps=[\n",
    "\t        ('preprocess', preprocess),\n",
    "\t        ('classifier', classifier)\n",
    "\t\t])\n",
    "\n",
    "\t\ttrain_acc = []\n",
    "\t\ttest_acc = []\n",
    "\t\ttrain_recall = []\n",
    "\t\ttest_recall = []\n",
    "\t\ttrain_precision = []\n",
    "\t\ttest_precision = []\n",
    "\t\ttrain_f1 = []\n",
    "\t\ttest_f1 = []\n",
    "\t\ttrain_auc_roc = []\n",
    "\t\ttest_auc_roc = []\n",
    "\t\tmean = []\n",
    "\t\t\n",
    "\t\tfor train_ind, val_ind in cv.split(X_train, y_train):\n",
    "\t\t\tX_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind]\n",
    "\t\t\tpipeline.fit(X_t, y_t)\n",
    "\t\t\ty_hat_t = pipeline.predict(X_t)\n",
    "\t\t\ttrain_acc.append(accuracy_score(y_t, y_hat_t))\n",
    "\t\t\ttrain_recall.append(recall_score(y_t, y_hat_t))\n",
    "\t\t\ttrain_precision.append(precision_score(y_t, y_hat_t))\n",
    "\t\t\ttrain_f1.append(f1_score(y_t, y_hat_t))\n",
    "\t\t\ttrain_auc_roc.append(roc_auc_score(y_t, y_hat_t))\n",
    "\t\t\tX_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind] \n",
    "\t\t\ty_hat_val = pipeline.predict(X_val)\n",
    "\t\t\ttest_acc.append(accuracy_score(y_val, y_hat_val))\n",
    "\t\t\ttest_recall.append(recall_score(y_val, y_hat_val))\n",
    "\t\t\ttest_precision.append(precision_score(y_val, y_hat_val))\n",
    "\t\t\ttest_f1.append(f1_score(y_val, y_hat_val))\n",
    "\t\t\ttest_auc_roc.append(roc_auc_score(y_val, y_hat_val))\n",
    "            \n",
    "\t\tmodel_result.append({\n",
    "            'classifier':classifier.__class__.__name__,\n",
    "            'scalerName':scaler.__class__.__name__,\n",
    "            'dataSampling':dataSamplingStatus,\n",
    "            'dtColumn':dtColumnStatus,\n",
    "            'incomePreprocessing':incomePreproStatus,\n",
    "            'IncomeEngineering':IncomeEngiStatus,\n",
    "            'yearProcess':yearProcessStatus,\n",
    "            'train_accuracy':np.mean(train_acc),\n",
    "            'test_accuracy':np.mean(test_acc),\n",
    "            'train_recall':np.mean(train_recall),\n",
    "            'test_recall':np.mean(test_recall),\n",
    "            'train_precision':np.mean(train_precision),\n",
    "            'test_precision':np.mean(test_precision),\n",
    "            'train_f1':np.mean(train_f1),\n",
    "            'test_f1':np.mean(test_f1),\n",
    "            'train_auc_roc':np.mean(train_auc_roc),\n",
    "            'test_auc_roc':np.mean(test_auc_roc)\n",
    "        })\n",
    "\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "\t#Lasso(),\n",
    "# \tSGDClassifier(max_iter = 1000, tol=1e-3,penalty = \"elasticnet\"),\n",
    "# \tLinearSVC(), \n",
    "# \tGaussianProcessClassifier(),\n",
    "# \tExtraTreesClassifier(), \n",
    "# # \tBernoulliNB(),\n",
    "\tLogisticRegressionCV(max_iter= 1200), \n",
    "# \tRidgeClassifierCV(),\n",
    "\tSVC(kernel = 'linear',max_iter= -1), \n",
    "# \tPerceptron(),\n",
    "# \tPassiveAggressiveClassifier(), \n",
    "# \tDecisionTreeClassifier(), #no coef \n",
    "# \tKNeighborsClassifier(),#no feat_import, use permutation_importance \n",
    "# \tGaussianNB(), #no feat_import, use permutation_importance \n",
    "\tLGBMClassifier(),#no coef \n",
    "\tRandomForestClassifier(), #no coef \n",
    "# \tGradientBoostingClassifier(),#no coef \n",
    "# \tPassiveAggressiveClassifier(), \n",
    "# \tExtraTreesClassifier(), #no coef \n",
    "\tXGBClassifier(),\n",
    "# \tAdaBoostClassifier(), #no coef\n",
    "# \tMLPClassifier() #mlp not working\n",
    "\t]\n",
    "tunable_cols = [\"Year_Birth\", \"Income\",\"Dt_Customer\"]\n",
    "model_result= []\n",
    "classes = [\"no\", \"yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "problemSet=None;\n",
    "counter=0;\n",
    "for processed_dt_cust in hyper_dt_customer:\n",
    "  dtColumnMethod,data= getKeyVal(processed_dt_cust)\n",
    "  for preprocessed_income in getPreprocessingIncome(data):\n",
    "    incomePreprocessMethod,data= getKeyVal(preprocessed_income)\n",
    "    for processed_income_col in getHypeIncome(data):\n",
    "      incomeEngineeringMethod,data= getKeyVal(processed_income_col)\n",
    "      for processed_year_birth in getHypeYearBirth(data):\n",
    "        processYearMethod,data = getKeyVal(processed_year_birth) # print(data)\n",
    "        numerical_bool_col = [x for x in data.columns if data[x].isin([0,1]).all()] # print(numerical_bool_col)\n",
    "        numerical_scalable_col = [x for x in data.columns if x not in numerical_bool_col]\n",
    "        y = data.Response # print(y.value_counts())\n",
    "        X = data.drop(['Response'], axis=1)\n",
    "        #should auto drop columns based on pearson correlation , feature importance\n",
    "        #X.drop(['NumStorePurchases','NumCatalogPurchases','MntFruits','MntFishProducts','MntSweetProducts','MntWines'], axis = 1, inplace = True)\n",
    "        \n",
    "\n",
    "    #             print(\"Dt                : \",dtColumnMethod,\"\\n\")\n",
    "    #             print(\"Preprocess Income : \",incomePreprocessMethod,\"\\n\")\n",
    "    #             print(\"Income Engineering: \",incomeEngineeringMethod,\"\\n\")\n",
    "    #             print(\"Process Year      : \",processYearMethod,\"\\n\") # print(numerical_scalable_col) # print(data.info())\n",
    "    #             print(\"Data Sampling     : \",dataSamplingDesc,\"\\n\")\n",
    "              \n",
    "      \n",
    "        for model in models:\n",
    "            x_dropped_data = X.drop([x for x in X.columns if x not in classifier_columns[model.__class__.__name__]],axis = 1)\n",
    "            nummerical_no_bool = [x for x in numerical_scalable_col if x in x_dropped_data.columns]\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(x_dropped_data,y, test_size=0.25, random_state=42)\n",
    "            for dataSamplingMethod in getDataSampling(X_train,y_train):\n",
    "              dataSamplingDesc,data= getKeyVal(dataSamplingMethod)\n",
    "              X_train,y_train = data\n",
    "#             print(X_train.isnull().values.any(),y_train.isnull().values.any(),\"\\n\")\n",
    "              if(X_train.isnull().values.any()==True):\n",
    "                  problemSet = X_train\n",
    "                  print(X_train.isnull())\n",
    "                  break\n",
    "              counter+=1\n",
    "              if(counter%100==0):\n",
    "                  print(\"Counter : \",counter)\n",
    "              model_result.append(cross_validate(model,StratifiedKFold(),X_train,y_train,dtColumnMethod,incomePreprocessMethod\n",
    "                                                    ,incomeEngineeringMethod,processYearMethod,dataSamplingDesc,nummerical_no_bool))\n",
    "  #                 overallClassificationReport(model,classes)\n",
    "#                 if hasmethod(model, 'predict_proba'):\n",
    "#                     ROC_Curve_Plot(model,X_test,y_test,\"Overall \"+model.__class__.__name__)\n",
    "#                 if has_feature_imp(model) :\n",
    "#                     overall_feature_importance(model,X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = pd.DataFrame.from_records(model_result,columns=['classifier','scalerName','dataSampling','dtColumn'\n",
    "                                                           ,'incomePreprocessing','IncomeEngineering','yearProcess'\n",
    "                                                           ,'train_accuracy','test_accuracy','train_recall','test_recall'\n",
    "                                                           ,'train_precision','test_precision','train_f1','test_f1',\n",
    "                                                          'train_roc_auc','test_roc_auc'])\n",
    "resultDF=resultDF.dropna(how='all')\n",
    "\n",
    "# roc_auc\n",
    "# f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultDF.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accColumn=['classifier','scalerName','dataSampling','dtColumn','incomePreprocessing','IncomeEngineering','yearProcess','train_accuracy','test_accuracy']\n",
    "resultDF[accColumn].sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recallColumn=['classifier','scalerName','dataSampling','dtColumn','incomePreprocessing','IncomeEngineering','yearProcess','train_recall','test_recall']\n",
    "resultDF[recallColumn].sort_values('test_recall',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precColumn=['classifier','scalerName','dataSampling','dtColumn','incomePreprocessing','IncomeEngineering','yearProcess','train_precision','test_precision']\n",
    "resultDF[precColumn].sort_values('test_precision',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1Column=['classifier','scalerName','dataSampling','dtColumn','incomePreprocessing','IncomeEngineering','yearProcess','train_f1','test_f1']\n",
    "resultDF[f1Column].sort_values('test_f1',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_aucColumn=['classifier','scalerName','dataSampling','dtColumn','incomePreprocessing','IncomeEngineering','yearProcess','train_roc_auc','test_roc_auc']\n",
    "resultDF[roc_aucColumn].sort_values('test_roc_auc',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc xgb, rfc, lgbm, logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scoring = 'f1'\n",
    "fold=10\n",
    "featureNumList = list(range(1,X_train.shape[1]))\n",
    "modelsWithParam = [\n",
    "         { \n",
    "        'model':LogisticRegression(),'param':{'C': np.logspace(-3,3,7),#100,10,1, 0.1, 0.01, 0.001\n",
    "                                 'fit_intercept':[True,False],\n",
    "                                 'dual':[True,False],\n",
    "                                 'penalty':['l2'],\n",
    "                                 'max_iter':list(range(100,1000,100)),#[50,100,500,1000,2000,4000,8000]\n",
    "                                 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "        }},\n",
    "    {\n",
    "        'model':LGBMClassifier(),'param':{\n",
    "        'boosting_type':[\"gbdt\",\"dart\",\"goss\",\"rf\"],\n",
    "        'metric':['binary_logloss'],\n",
    "        'sub_feature':list(np.arange(0.1,1,10)),\n",
    "        'num_leaves':list(range(10,50,10)),\n",
    "        'learning_rate': [1,0.1,0.01,0.005,0.001],\n",
    "        'n_estimators': list(range(100,1000,100)),\n",
    "        'min_data':[50],\n",
    "        'max_depth': list(range(5,20,5)),\n",
    "        'min_split_gain':list(np.arange(0.1,1,10)),\n",
    "        'random_state': [42]\n",
    "    }},\n",
    "    {\n",
    "        'model':RandomForestClassifier(),'param':{\n",
    "            'bootstrap': [True, False],\n",
    "             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': [1, 2, 4],\n",
    "             'min_samples_split': [2, 5, 10],\n",
    "             'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "    },\n",
    "    {\n",
    "        'model':XGBClassifier(),'param':{\n",
    "            'n_estimators': [400, 700, 1000],\n",
    "            'colsample_bytree': [0.7, 0.8],\n",
    "            'max_depth': [15,20,25],\n",
    "            'reg_alpha': [1.1, 1.2, 1.3],\n",
    "            'reg_lambda': [1.1, 1.2, 1.3],\n",
    "            'subsample': [0.7, 0.8, 0.9]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "            \n",
    "        'model':SVC(),'param':{\n",
    "        'C': [ 1,0.1,10, 100, 1000],  \n",
    "        'gamma': [0.0001,0,00001], \n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'random_state': [42] \n",
    "    }},\n",
    "    \n",
    "    \n",
    "    \n",
    "#     { split svc into 2 as svc poly and non-poly kernel has\n",
    "#         #different parameter\n",
    "#         'model':SVC(),'param':{\n",
    "#         'C': [0.1, 1, 10, 100, 1000],  \n",
    "#         'gamma': [1,0.1,0.01,0.005,0.001,0.0005,0.0001], \n",
    "#         'kernel': ['poly'],\n",
    "#         'degree':list(range(3,10)),\n",
    "#         'random_state': [42],\n",
    "#     }}\n",
    "\n",
    "#         {\n",
    "#         'model':LogisticRegression(),'param':{'Cs': [[100,10,1, 0.1,0.05,0.001,0.0001]],#100,10,1, 0.1, 0.01, 0.001\n",
    "#                                  'fit_intercept':[True,False],\n",
    "#                                  'normalize':[True,False],\n",
    "#                                  'penalty':['elasticnet'],\n",
    "#                                  'penalty':[True],\n",
    "#                                  'max_iter':list(range(100,1000,100)),#[50,100,500,1000,2000,4000,8000]\n",
    "#                                  'solver':['saga'],\n",
    "#                                  'random_state':[42]\n",
    "#         }}\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines column to drop for each classifier\n",
    "classifier_columns = {\n",
    "\t\t\"SVC\":[\"AcceptedCmp5\",\"AcceptedCmp2\",\"Complain\",\"enroll_year\",\"NumCatalogPurchases\",\"Year_Birth\",\"Marital_Married\",\"MntGoldProds\"],\n",
    "\t\t\"LGBMClassifier\":[\"Kidhome\",\"MntWines\",\"Education\",\"AcceptedCmp4\",\"MntFishProducts\",\"MntMeatProducts\",\"NumStorePurchases\",\"Teenhome\"],\n",
    "\t\t\"RandomForestClassifier\":[\"Kidhome\",\"Teenhome\",\"Education\",\"MntWines\",\"MntFishProducts\",\"AcceptedCmp4\",\"AcceptedCmp5\",\"AcceptedCmp2\"],\n",
    "\t\t\"XGBClassifier\":[\"AcceptedCmp2\",\"AcceptedCmp5\",\"Complain\",\"Kidhome\",\"Year_Birth\",\"NumDealsPurchases\",\"Marital_Married\",\"Marital_Together\"],\n",
    "\t\t\"LogisticRegression\":[\"Complain\",\"AcceptedCmp2\",\"AcceptedCmp5\",\"Year_Birth\",\"NumCatalogPurchases\",\"enroll_year\",\"Marital_Married\",\"MntGoldProds\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((559, 37), (560,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#randomised search cv\n",
    "fold=10\n",
    "def modelBestFit(item,X_trains,y_trains,X_tests,y_tests):\n",
    "    model = item['model']\n",
    "    paramGrid = item['param']\n",
    "    search = RandomizedSearchCV(estimator =model, param_distributions= paramGrid\n",
    "                                ,n_iter=100,n_jobs=-1,pre_dispatch='1*n_jobs'\n",
    "                                ,scoring = scoring,refit = True,cv=fold\n",
    "                                ,random_state=42)\n",
    "    search.fit(X_trains,y_trains.values)\n",
    "    \n",
    "    best_score =search.score(X_tests,y_tests.values)\n",
    "    model_name = model.__class__.__name__\n",
    "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\n",
    "\n",
    "def bestModel(modelsAndParams):\n",
    "    modelPerformance = pd.DataFrame()\n",
    "    for item in modelsAndParams:\n",
    "        \n",
    "        model = item['model']\n",
    "        \n",
    "        x_copy = X.copy()\n",
    "        x_copy = x.drop([x for x in x_copy.columns if x not in classifier_columns[model.__class__.__name__]],axis = 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "        \n",
    "        X_train= X_train.fillna(method = \"ffill\")\n",
    "        X_test = X_test.fillna(method = \"ffill\")\n",
    "        \n",
    "        X_train = extractFromDate(X_train)\n",
    "        X_test = extractFromDate(X_test)\n",
    "        \n",
    "        X_train = getNormalizedAndBinnedIncome(X_train)\n",
    "        X_test = getNormalizedAndBinnedIncome(X_test)\n",
    "        \n",
    "        y_train = y_train[y_train.index.isin(X_train.index)]\n",
    "        y_test = y_test[y_test.index.isin(X_test.index)]\n",
    "        \n",
    "        \n",
    "        qt = QuantileTransformer(random_state=42)\n",
    "        X_train = qt.fit_transform(X_train)\n",
    "        X_test = qt.transform(X_test)\n",
    "        \n",
    "        \n",
    "        nystroem = Nystroem()\n",
    "        \n",
    "        X_train_transformed = nystroem.fit_transform(X_train)\n",
    "        X_test_transformed = nystroem.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "        result = modelBestFit(item,x_copy,y_train)\n",
    "        modelPerformance = modelPerformance.append(result,ignore_index=True)\n",
    "        print(result)\n",
    "        \n",
    "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\n",
    "    return modelPerformance\n",
    " \n",
    "result = bestModel(modelsWithParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NumDealsPurchases', 'AcceptedCmp5', 'AcceptedCmp2', 'Complain',\n",
      "       'Marital_Married'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TuneSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3ea269b8eab2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodelPerformance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelsWithParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-3ea269b8eab2>\u001b[0m in \u001b[0;36mbestModel\u001b[1;34m(modelsAndParams)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mx_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifier_columns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelBestFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_copy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mmodelPerformance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelPerformance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-3ea269b8eab2>\u001b[0m in \u001b[0;36mmodelBestFit\u001b[1;34m(item, X_train, y_train)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mparamGrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'param'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     search = TuneSearchCV(model,\n\u001b[0m\u001b[0;32m     10\u001b[0m        \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m        \u001b[0mparam_distributions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparamGrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TuneSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "#bayesian optimised search cv\n",
    "fold=10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def modelBestFit(item,X_train,y_train):\n",
    "    model = item['model']\n",
    "    paramGrid = item['param']\n",
    "    search = TuneSearchCV(model,\n",
    "       n_jobs=-1,\n",
    "       param_distributions=paramGrid,\n",
    "       early_stopping=True,\n",
    "       search_optimization=\"bayesian\")\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    print(search.best_params_)\n",
    "    #need to make sure the score is done by best model\n",
    "    best_score =search.score(X_test,y_test.values)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\n",
    "\n",
    "def bestModel(modelsAndParams):\n",
    "    modelPerformance = pd.DataFrame()\n",
    "    for item in modelsAndParams:\n",
    "        \n",
    "        model = item['model']\n",
    "        x_copy = X_train.copy()\n",
    "        x_copy = x_copy.drop([x for x in x_copy.columns if x not in classifier_columns[model.__class__.__name__]],axis = 1)\n",
    "        print(x_copy.columns)\n",
    "        result = modelBestFit(item,x_copy,y_train)\n",
    "        \n",
    "        modelPerformance = modelPerformance.append(result,ignore_index=True)\n",
    "        \n",
    "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\n",
    "    return modelPerformance\n",
    " \n",
    "result = bestModel(modelsWithParam)\n",
    "warnings.filterwarnings(\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
