{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "MachineLearningV2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7m7KcsSWdnR"
      },
      "source": [
        "#3rd party libraries\r\n",
        "#!pip install pandas_bokeh\r\n",
        "#!pip install bayesian-optimization\r\n",
        "#!pip install six\r\n",
        "#!pip install eli5\r\n",
        "#!pip install yellowbrick\r\n",
        "#!pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXnMFrQ-NT5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f73202f-7b95-4296-d56a-ba0f44f1b79f"
      },
      "source": [
        "# Libraries\r\n",
        "import pandas as pd  \r\n",
        "import numpy as np  \r\n",
        "import matplotlib.pyplot as plt  \r\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV,ParameterGrid\r\n",
        "from sklearn.linear_model import Perceptron, LogisticRegressionCV, RidgeClassifierCV, SGDClassifier, PassiveAggressiveClassifier, RidgeClassifier\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score,mean_absolute_error, confusion_matrix\r\n",
        "from sklearn.metrics import roc_auc_score,roc_curve, auc, classification_report,precision_score,recall_score,log_loss,f1_score\r\n",
        "from sklearn.feature_selection import RFE\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "import xgboost as xgb\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from scipy.stats import uniform, randint\r\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, AdaBoostClassifier\r\n",
        "from bayes_opt import BayesianOptimization\r\n",
        "from lightgbm import LGBMClassifier\r\n",
        "from imblearn.over_sampling import SMOTE\r\n",
        "from sklearn import tree\r\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\r\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, LabelEncoder, OneHotEncoder, MaxAbsScaler, RobustScaler, QuantileTransformer\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn import tree\r\n",
        "import pandas_bokeh\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.calibration import CalibratedClassifierCV\r\n",
        "from numpy import mean, std\r\n",
        "import pandas.testing as tm\r\n",
        "from scipy import stats\r\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.inspection import permutation_importance\r\n",
        "from skopt import BayesSearchCV\r\n",
        "\r\n",
        "# Pipelines\r\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "\r\n",
        "#other\r\n",
        "from math import sqrt\r\n",
        "import inspect\r\n",
        "from matplotlib.font_manager import FontProperties\r\n",
        "from scipy.stats import loguniform, uniform\r\n",
        "\r\n",
        "import eli5\r\n",
        "\r\n",
        "from yellowbrick.features import Rank2D\r\n",
        "from yellowbrick.features import PCA as PCA_YB\r\n",
        "from yellowbrick.features.radviz import RadViz\r\n",
        "from yellowbrick.features import pca_decomposition\r\n",
        "from yellowbrick.features import Manifold\r\n",
        "from yellowbrick.features import JointPlotVisualizer\r\n",
        "from yellowbrick.classifier import ClassificationReport\r\n",
        "from yellowbrick.classifier import PrecisionRecallCurve\r\n",
        "from yellowbrick.classifier import ClassPredictionError\r\n",
        "from yellowbrick.model_selection import LearningCurve\r\n",
        "from yellowbrick.model_selection import CVScores\r\n",
        "from yellowbrick.model_selection import FeatureImportances\r\n",
        "from yellowbrick.features import ParallelCoordinates\r\n",
        "from yellowbrick.model_selection import RFECV\r\n",
        "from yellowbrick.classifier import ROCAUC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ckMkNrcNVg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7cfc62-cfab-49dd-9784-8093c6663ec4"
      },
      "source": [
        "DATASET_URL = \"https://gist.githubusercontent.com/YHYeoh/ad1a7f7170c72d621d05a70637540152/raw/5a6059c199e2c46d2f3d258f03d93cfea98e2749/marketing_campaign.csv\"\r\n",
        "data = pd.read_csv(DATASET_URL, sep = ';')\r\n",
        "\r\n",
        "pd.set_option('plotting.backend','pandas_bokeh')\r\n",
        "\r\n",
        "data.fillna(method = \"ffill\", inplace = True)\r\n",
        "data.isnull().values.any()\r\n",
        "\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "enc = OneHotEncoder()\r\n",
        "data[\"Education\"] = label_encoder.fit_transform(data[\"Education\"])\r\n",
        "print(label_encoder.classes_)\r\n",
        "# enc_df = pd.DataFrame(enc.fit_transform(data[[\"Marital_Status\"]]).toarray())\r\n",
        "# print(enc.get_feature_names())\r\n",
        "# data = data.join(enc_df)\r\n",
        "\r\n",
        "marital_status_ohe = pd.get_dummies(data[\"Marital_Status\"],prefix=\"Marital\")\r\n",
        "ohe_cols = marital_status_ohe.columns\r\n",
        "data = pd.concat([data, marital_status_ohe], axis=1)\r\n",
        "\r\n",
        "\r\n",
        "data['enroll_year'] = pd.DatetimeIndex(data.Dt_Customer).year\r\n",
        "data['enroll_month'] = pd.DatetimeIndex(data.Dt_Customer).month\r\n",
        "data['enroll_day'] = pd.DatetimeIndex(data.Dt_Customer).day\r\n",
        "\r\n",
        "data.drop([\"ID\", 'Dt_Customer',\"Z_CostContact\",\"Z_Revenue\",\"Marital_Status\"], axis=1, inplace=True)\r\n",
        "\r\n",
        "categorical = ['Marital_Status']\r\n",
        "numerical = ['Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\r\n",
        "       'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\r\n",
        "       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\r\n",
        "       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\r\n",
        "       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\r\n",
        "       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',\r\n",
        "       'Complain', 'enroll_year', 'enroll_month', 'enroll_day']\r\n",
        "numerical_no_bool = ['Education','Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth','enroll_day','enroll_month','enroll_year']\r\n",
        "\r\n",
        "y = data.Response\r\n",
        "X = data.drop(['Response'], axis=1)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2n Cycle' 'Basic' 'Graduation' 'Master' 'PhD']\n",
            "Index(['Year_Birth', 'Education', 'Income', 'Kidhome', 'Teenhome', 'Recency',\n",
            "       'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
            "       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
            "       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
            "       'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5',\n",
            "       'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Marital_Absurd',\n",
            "       'Marital_Alone', 'Marital_Divorced', 'Marital_Married',\n",
            "       'Marital_Single', 'Marital_Together', 'Marital_Widow', 'Marital_YOLO',\n",
            "       'enroll_year', 'enroll_month', 'enroll_day'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkPBcz5KNZi6"
      },
      "source": [
        "def hasmethod(obj, name):\r\n",
        "    return inspect.ismethod(getattr(obj, name, None))\r\n",
        "\r\n",
        "def ROC_Curve_Plot(model,X_test,y_test):\r\n",
        "    predProb = model.predict_proba(X_test)\r\n",
        "    preds = predProb[:,1]\r\n",
        "    fpr, tpr, threshold = roc_curve(y_test, preds,pos_label=1)\r\n",
        "    roc_auc = auc(fpr, tpr)\r\n",
        "    plt.close()\r\n",
        "    plt.title('Receiver Operating Characteristic')\r\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\r\n",
        "    plt.legend(loc = 'lower right')\r\n",
        "    plt.plot([0, 1], [0, 1],'r--')\r\n",
        "    #plt.xlim([0, 1])\r\n",
        "    #plt.ylim([0, 1])\r\n",
        "    plt.ylabel('True Positive Rate')\r\n",
        "    plt.xlabel('False Positive Rate')\r\n",
        "    plt.show()\r\n",
        "    return fpr,tpr\r\n",
        "\r\n",
        "def setupPreprocessPipeline(scaler):\r\n",
        "\tss = Pipeline(steps=[('scaler',scaler)])\r\n",
        "\t#ohe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown = 'ignore'))])\r\n",
        "\tpreprocess = ColumnTransformer(\r\n",
        "                    transformers=[\r\n",
        "                        ('cont', ss, numerical_no_bool)\r\n",
        "                        #('cat', ohe, categorical),\r\n",
        "                        #('le', le, ordinal),\r\n",
        "                        ],remainder='passthrough')\r\n",
        "\treturn preprocess\r\n",
        "\r\n",
        "def feature_importance(classifier, feature_names, scaler_name):\r\n",
        "\tif (hasattr(classifier,'coef_')):\r\n",
        "\t\timportance = classifier.coef_[0]\r\n",
        "\telif (hasattr(classifier,'coefs_')):\r\n",
        "\t\timportance = classifier.coefs_\r\n",
        "\telif (hasattr(classifier,'feature_importances_')):\r\n",
        "\t\timportance = classifier.feature_importances_\r\n",
        "\telse:\r\n",
        "\t\tprint(\"Cannot extract feature importance, skipping\")\r\n",
        "\t\treturn\r\n",
        "\r\n",
        "\tfor i,v in enumerate(importance):\r\n",
        "\t\tprint('Feature: %d, Score: %.5f' % (i,v))\r\n",
        "\tzipped = zip(feature_names, importance)\r\n",
        "\tdf = pd.DataFrame(zipped, columns=[\"feature\", \"value\"])\r\n",
        "\t# Sort the features by the absolute value of their coefficient\r\n",
        "\tdf[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\r\n",
        "\tdf[\"colors\"] = df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\r\n",
        "\tdf = df.sort_values(\"abs_value\", ascending=False)\r\n",
        "\t# plot feature importance\r\n",
        "\tfig, ax = plt.subplots(1, 1, figsize=(16, 9))\r\n",
        "\tsns.barplot(x=\"feature\",\r\n",
        "\t            y=\"value\",\r\n",
        "\t            data=df.head(20),\r\n",
        "\t           palette=df.head(20)[\"colors\"])\r\n",
        "\tplt.gcf().subplots_adjust(bottom=0.30)\r\n",
        "\tax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=14)\r\n",
        "\tax.set_title(\"Top 20 Features for {} w/ {}\".format(classifier.__class__.__name__, scaler_name), fontsize=25)\r\n",
        "\tax.set_ylabel(\"Coef\", fontsize=22)\r\n",
        "\tax.set_xlabel(\"Feature Name\", fontsize=22)\r\n",
        "\tplt.show()\r\n",
        "\r\n",
        "def evaluation(y, y_hat, title):\r\n",
        "    cm = confusion_matrix(y, y_hat)\r\n",
        "    precision = precision_score(y, y_hat)\r\n",
        "    recall = recall_score(y, y_hat)\r\n",
        "    accuracy = accuracy_score(y,y_hat)\r\n",
        "    f1 = f1_score(y,y_hat)\r\n",
        "    print('Recall: ', recall)\r\n",
        "    print('Accuracy: ', accuracy)\r\n",
        "    print('Precision: ', precision)\r\n",
        "    print('F1: ', f1)\r\n",
        "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\r\n",
        "    plt.xlabel('predicted', fontsize=18)\r\n",
        "    plt.ylabel('actual', fontsize=18)\r\n",
        "    plt.title(title, fontsize=18)\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "def metrics_summary(y_test,y_pred):\r\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\r\n",
        "\r\n",
        "    accuracy=accuracy_score(y_test, y_pred)\r\n",
        "    precision = precision_score(y_test, y_pred)\r\n",
        "    recall =  recall_score(y_test, y_pred) #sensitivity\r\n",
        "    specificity = tn / (tn+fp)\r\n",
        "    g_mean= sqrt(recall * specificity)\r\n",
        "    mse =mean_squared_error(y_test, y_pred, squared=False)\r\n",
        "    r2=r2_score(y_test, y_pred)\r\n",
        "    ros = roc_auc_score(y_test, y_pred)\r\n",
        "    ll = log_loss(y_test, y_pred)\r\n",
        "    f1 = f1_score(y_test, y_pred)\r\n",
        "\r\n",
        "    metrics_collection_dict ={\r\n",
        "        'accuracy':['accuracy',accuracy],\r\n",
        "        'precision':['precision',precision],\r\n",
        "        'recall':['recall',recall],\r\n",
        "        'specificity':['specificity',specificity],\r\n",
        "        'g_mean':['g_mean',g_mean],\r\n",
        "        'mean_square_error':['mean_square_error',mse],\r\n",
        "        'r2':['r2',r2],\r\n",
        "        'roc_auc_score':['roc_auc_score',ros],\r\n",
        "        'log_loss':['log_loss',ll],\r\n",
        "        'f1_score':['f1_score',f1]\r\n",
        "    } \r\n",
        "    return metrics_collection_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxAAHzryNskR"
      },
      "source": [
        "def cross_validate(classifier, cv):\r\n",
        "\tscalers = [StandardScaler(),MinMaxScaler(),MaxAbsScaler(), RobustScaler()]\r\n",
        "\ttrain_acc = []\r\n",
        "\ttest_acc = []\r\n",
        "\tmean = []\r\n",
        "\tresult = []\r\n",
        "\tfor scaler in scalers:\r\n",
        "\t\tfpr = None\r\n",
        "\t\ttpr = None\r\n",
        "\t\tpreprocess = setupPreprocessPipeline(scaler)\r\n",
        "\t\tpipeline = Pipeline(steps=[\r\n",
        "\t        ('preprocess', preprocess),\r\n",
        "\t        ('classifier', classifier)\r\n",
        "    \t])\r\n",
        "\r\n",
        "\t\ttrain_acc = []\r\n",
        "\t\ttest_acc = []\r\n",
        "\t\tmean = []\r\n",
        "\r\n",
        "\t\tfor train_ind, val_ind in cv.split(X_train, y_train):\r\n",
        "\t\t\tX_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind]\r\n",
        "\t\t\tpipeline.fit(X_t, y_t)\r\n",
        "\t\t\ty_hat_t = pipeline.predict(X_t)\r\n",
        "\t\t\ttrain_acc.append(accuracy_score(y_t, y_hat_t))\r\n",
        "\t\t\tX_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind] \r\n",
        "\t\t\ty_hat_val = pipeline.predict(X_val)\r\n",
        "\t\t\ttest_acc.append(accuracy_score(y_val, y_hat_val))\r\n",
        "\r\n",
        "\t\t#ohe_cols = list(pipeline.named_steps['preprocess'].named_transformers_['cat'].named_steps['ohe'].get_feature_names(input_features=categorical))\r\n",
        "\t\tnumeric_feature_list = list(numerical)\r\n",
        "\t\tfor i in ohe_cols:\r\n",
        "\t\t\tnumeric_feature_list.append(i)\r\n",
        "\t\tprint(len(numeric_feature_list))\r\n",
        "\t\tevaluation(y_val, y_hat_val, 'Confusion Matrix {} + {}'.format(classifier.__class__.__name__, scaler.__class__.__name__).strip())\r\n",
        "\t\tprint('Mean Training Accuracy: {} | Standard Deviation: {}'.format(np.mean(train_acc),np.std(test_acc)))\r\n",
        "\t\tprint('Mean Validation Accuracy: {} | Standard Deviation: {}'.format(np.mean(test_acc),np.std(test_acc)))\r\n",
        "\t\tprint('\\n')\r\n",
        "        \r\n",
        "\t\tfeature_importance(classifier, numeric_feature_list, scaler.__class__.__name__ )\r\n",
        "\t\tmetrics_summ = metrics_summary(y_val,y_hat_val)\r\n",
        "\t\tif hasmethod(pipeline['classifier'], 'predict_proba'):\r\n",
        "\t\t\tfpr,tpr = ROC_Curve_Plot(pipeline,X_val,y_val)\r\n",
        "\t\tresult.append({\r\n",
        "            'classifier':classifier.__class__.__name__,\r\n",
        "            'scalerName':scaler.__class__.__name__,\r\n",
        "            'metrics_summ':metrics_summ,\r\n",
        "            'fpr':fpr,\r\n",
        "            'tpr':tpr\r\n",
        "        })\r\n",
        "\treturn result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yjPnG_fn3XJ"
      },
      "source": [
        "models = [ \r\n",
        "\tLogisticRegressionCV(max_iter= 1200), \r\n",
        "\tRidgeClassifierCV(),\r\n",
        "\tSVC(kernel = 'linear',max_iter= -1), \r\n",
        "\tPerceptron(),\r\n",
        "\tPassiveAggressiveClassifier(), \r\n",
        "\tDecisionTreeClassifier(), #no coef \r\n",
        "\tKNeighborsClassifier(),#no feat_import, use permutation_importance \r\n",
        "\tGaussianNB(), #no feat_import, use permutation_importance \r\n",
        "\tLGBMClassifier(),#no coef \r\n",
        "\tRandomForestClassifier(), #no coef \r\n",
        "\tGradientBoostingClassifier(),#no coef \r\n",
        "\tPassiveAggressiveClassifier(), \r\n",
        "\tExtraTreesClassifier(), #no coef \r\n",
        "\tXGBClassifier(),\r\n",
        "\tAdaBoostClassifier(), #no coef \r\n",
        "\t]\r\n",
        "\r\n",
        "model_result = []\r\n",
        "\r\n",
        "for model in models:\r\n",
        "\tprint(model.__class__.__name__)\r\n",
        "\tmodel_result.append(cross_validate(model,KFold()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzI5diiIn3OT"
      },
      "source": [
        "standardScalerList = []\r\n",
        "minMaxScalerList = []\r\n",
        "maxAbsScalerList = []\r\n",
        "robustScalerList = []\r\n",
        "for collection in model_result:\r\n",
        "    standard = collection[0]\r\n",
        "    standardScalerList.append({'classifier':standard['classifier'],'metrics_summ':standard['metrics_summ'],'fpr':standard['fpr'],'tpr':standard['tpr'] })\r\n",
        "    minMax = collection[1]\r\n",
        "    minMaxScalerList.append({'classifier':minMax['classifier'],'metrics_summ':minMax['metrics_summ'],'fpr':minMax['fpr'],'tpr':minMax['tpr'] })\r\n",
        "    maxAbs = collection[2]\r\n",
        "    maxAbsScalerList.append({'classifier':maxAbs['classifier'],'metrics_summ':maxAbs['metrics_summ'],'fpr':maxAbs['fpr'],'tpr':maxAbs['tpr'] })\r\n",
        "    robust = collection[3]\r\n",
        "    robustScalerList.append({'classifier':robust['classifier'],'metrics_summ':robust['metrics_summ'],'fpr':robust['fpr'],'tpr':robust['tpr'] })\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VHwPTf6n3ER"
      },
      "source": [
        "nameList = []\r\n",
        "\r\n",
        "for object in model_result:\r\n",
        "    nameList.append(object[0]['classifier'])\r\n",
        "metric_list = ['accuracy','precision','recall','specificity','g_mean'\r\n",
        "                   ,'mean_square_error','r2','roc_auc_score','log_loss','f1_score']\r\n",
        "scaler = ['Standard Scaler','Min Max Scaler','Max Abs Scaler','robust Scaler']\r\n",
        "for metric in metric_list:\r\n",
        "    resultList = []\r\n",
        "    for model in model_result:\r\n",
        "        resultList.append(model[0]['metrics_summ'][metric][1])\r\n",
        "        \r\n",
        "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\r\n",
        "    plt.title(\"Models' \"+metric+ \" with Standard Scaler\")\r\n",
        "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiDEyUDbn26U"
      },
      "source": [
        "for metric in metric_list:\r\n",
        "    resultList = []\r\n",
        "    for model in model_result:\r\n",
        "        resultList.append(model[1]['metrics_summ'][metric][1])\r\n",
        "        \r\n",
        "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\r\n",
        "    plt.title(\"Models' \"+metric+ \" with Min Max Scaler\")\r\n",
        "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSsQdWSyn2gc"
      },
      "source": [
        "for metric in metric_list:\r\n",
        "    resultList = []\r\n",
        "    for model in model_result:\r\n",
        "        resultList.append(model[2]['metrics_summ'][metric][1])\r\n",
        "        \r\n",
        "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\r\n",
        "    plt.title(\"Models' \"+metric+ \" with Max Abs Scaler\")\r\n",
        "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeGnWf1goXwc"
      },
      "source": [
        "\r\n",
        "for metric in metric_list:\r\n",
        "    resultList = []\r\n",
        "    for model in model_result:\r\n",
        "        resultList.append(model[3]['metrics_summ'][metric][1])\r\n",
        "        \r\n",
        "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\r\n",
        "    plt.title(\"Models' \"+metric+ \" with Robust Scaler\")\r\n",
        "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Kj_TKMm84g"
      },
      "source": [
        "#Grid search params\r\n",
        "scoring = 'f1'\r\n",
        "fold=10\r\n",
        "featureNumList = list(range(1,X_train.shape[1]))\r\n",
        "modelsWithParam = [\r\n",
        "    {\r\n",
        "        'model':GradientBoostingClassifier(),'param': {\r\n",
        "        'loss':['deviance', 'exponential'],\r\n",
        "        'learning_rate': [0.1,0.01,0.005,0.001,0.0005,0.0001],\r\n",
        "        'min_samples_split': [0.1, 0.5, 5.0,10.0,20.0,40.0,80.0],\r\n",
        "        'min_samples_leaf': [0.1, 0.5, 5,10,20,40,80],\r\n",
        "        'max_depth':[5,10, 20, 30, 40, 60, 100, None],\r\n",
        "        'max_features':featureNumList,\r\n",
        "        'subsample':['0.8','1','1.2','1.4','1.6'],\r\n",
        "        'random_state': [42]\r\n",
        "        }}\r\n",
        "    ,{\r\n",
        "        'model':RandomForestClassifier(),'param':{'bootstrap': [True, False],\r\n",
        "        'max_depth': [5,10, 20, 30, 40, 60, 100, None],\r\n",
        "        'max_features': featureNumList,\r\n",
        "        'min_samples_leaf': [1, 2, 4],\r\n",
        "        'min_samples_split': [0.1, 0.5, 5.0,10.0,20.0,40.0,80.0],\r\n",
        "        'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\r\n",
        "        'random_state': [42]\r\n",
        "        }\r\n",
        "},{\r\n",
        "        'model':LGBMClassifier(),'param':{\r\n",
        "        'learning_rate': [0.1,0.01,0.005,0.001,0.0005,0.0001],\r\n",
        "        'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],\r\n",
        "        'max_depth': [5,10, 20, 30, 40, 60, 100, None],\r\n",
        "        'min_samples_split': [0.1, 0.5, 5.0,10.0,20.0,40.0,80.0],\r\n",
        "        'min_samples_leaf':[0.1, 0.5, 5,10,20,40,80],\r\n",
        "        'random_state': [42]\r\n",
        "    }},{\r\n",
        "            \r\n",
        "        'model':SVC(),'param':{\r\n",
        "        'C': [0.1, 1, 10, 100, 1000],  \r\n",
        "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \r\n",
        "        'kernel': ['linear', 'rbf', 'sigmoid', 'precomputed'],\r\n",
        "        'random_state': [42] \r\n",
        "    }},\r\n",
        "    {\r\n",
        "        'model':SVC(),'param':{\r\n",
        "        'C': [0.1, 1, 10, 100, 1000],  \r\n",
        "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \r\n",
        "        'kernel': ['poly'],\r\n",
        "        'degree':[3,4,5,6],\r\n",
        "        'random_state': [42] \r\n",
        "    }},{\r\n",
        "        'model':RidgeClassifier(),'param':{'alpha': [100,10,1, 0.1,0.05,0.001,0.0005],#100,10,1, 0.1, 0.01, 0.001\r\n",
        "                                 'fit_intercept':[True,False],\r\n",
        "                                 'normalize':[True,False],\r\n",
        "                                 'copy_X':[True],\r\n",
        "                                 'max_iter':[1,5,10,20,40,80,160,320,740,1480],#[50,100,500,1000,2000,4000,8000]\r\n",
        "                                 'solver':['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\r\n",
        "                                 'random_state':[42]\r\n",
        "        }}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoNS6Nn8m_cf"
      },
      "source": [
        "#gridsearchcv\r\n",
        "def modelBestFit(item):\r\n",
        "    model = item['model']\r\n",
        "    paramGrid = item['param']\r\n",
        "    search = GridSearchCV(model, paramGrid,n_jobs=-1,pre_dispatch='1*n_jobs'\r\n",
        "    ,scoring = scoring,refit = True,cv=fold)\r\n",
        "    search.fit(X_train,y_train)\r\n",
        "    best_score =search.score(X_test,y_test)\r\n",
        "    model_name = model.__class__.__name__\r\n",
        "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\r\n",
        "\r\n",
        "def bestModel(modelsAndParams):\r\n",
        "    modelPerformance = pd.DataFrame()\r\n",
        "    for item in modelsAndParams:\r\n",
        "        result = modelBestFit(item)\r\n",
        "        \r\n",
        "        modelPerformance = modelPerformance.append(result,ignore_index=True)\r\n",
        "        \r\n",
        "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\r\n",
        "    return modelPerformance\r\n",
        "result = bestModel(modelsWithParam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlKtak7OuE9C"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7W2RKjYrxwH"
      },
      "source": [
        "#randomised search and bayesian search params\r\n",
        "scoring = 'f1'\r\n",
        "fold = 10\r\n",
        "featureNumList = list(range(1,X_train.shape[1]))\r\n",
        "modelsWithParam = [ {\r\n",
        "        'model':GradientBoostingClassifier(),'param': {\r\n",
        "        'loss':['deviance', 'exponential'],\r\n",
        "        'learning_rate': loguniform(1e-5, 100),\r\n",
        "        'min_samples_split': loguniform(1e-5, 100),\r\n",
        "        'min_samples_leaf': loguniform(1e-5, 100),\r\n",
        "        'max_depth':list(range(5,100)),\r\n",
        "        'max_features':featureNumList,\r\n",
        "        'subsample':loguniform(0.1, 2),\r\n",
        "        'random_state': [42]\r\n",
        "        }}\r\n",
        "    ,\r\n",
        "   {\r\n",
        "        'model':RandomForestClassifier(),'param':{'bootstrap': [True, False],\r\n",
        "        'max_depth': list(range(5,100)),\r\n",
        "        'max_features': featureNumList,\r\n",
        "        'min_samples_leaf': loguniform(1e-5, 100),\r\n",
        "        'min_samples_split': loguniform(1e-5, 100),\r\n",
        "        'n_estimators': list(range(1,5000)),\r\n",
        "        'random_state': [42]\r\n",
        "        }\r\n",
        "},{\r\n",
        "        'model':LGBMClassifier(),'param':{\r\n",
        "        'learning_rate': loguniform(1e-5, 100),\r\n",
        "        'n_estimators': list(range(1,200)),\r\n",
        "        'max_depth': list(range(5,100)),\r\n",
        "        'min_samples_split': loguniform(1e-5, 100),\r\n",
        "        'min_samples_leaf':loguniform(1e-5, 100),\r\n",
        "        'random_state': [42]\r\n",
        "    }},{\r\n",
        "            \r\n",
        "        'model':SVC(),'param':{\r\n",
        "        'C': loguniform(1e-5, 10),  \r\n",
        "        'gamma': loguniform(1e-5, 100), \r\n",
        "        'kernel': ['linear', 'rbf', 'sigmoid', 'precomputed'],\r\n",
        "        'random_state': [42] \r\n",
        "    }},\r\n",
        "    {\r\n",
        "        'model':SVC(),'param':{\r\n",
        "        'C': loguniform(1e-5, 100),  \r\n",
        "        'gamma': loguniform(1e-5, 100), \r\n",
        "        'kernel': ['poly'],\r\n",
        "        'degree':list(range(3,12)),\r\n",
        "        'random_state': [42] \r\n",
        "    }},{\r\n",
        "        'model':RidgeClassifier(),'param':{'alpha': loguniform(1e-5, 100),#100,10,1, 0.1, 0.01, 0.001\r\n",
        "                                 'fit_intercept':[True,False],\r\n",
        "                                 'normalize':[True,False],\r\n",
        "                                 'copy_X':[True],\r\n",
        "                                 'max_iter':list(range(1,5000)),#[50,100,500,1000,2000,4000,8000]\r\n",
        "                                 'solver':['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\r\n",
        "                                 'random_state':[42]\r\n",
        "        }}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K44wVzO3r2iw"
      },
      "source": [
        "#randomised search cv\r\n",
        "def modelBestFit(item):\r\n",
        "    model = item['model']\r\n",
        "    paramGrid = item['param']\r\n",
        "    search = RandomizedSearchCV(model, paramGrid,n_jobs=-1,pre_dispatch='1*n_jobs',scoring = scoring,refit = True,cv=fold)\r\n",
        "    search.fit(X_train,y_train)\r\n",
        "    best_score =search.score(X_test,y_test)\r\n",
        "    model_name = model.__class__.__name__\r\n",
        "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\r\n",
        "\r\n",
        "def bestModel(modelsAndParams):\r\n",
        "    modelPerformance = pd.DataFrame()\r\n",
        "    for item in modelsAndParams:\r\n",
        "        result = modelBestFit(item)\r\n",
        "        \r\n",
        "        modelPerformance = modelPerformance.append(result,ignore_index=True)\r\n",
        "        \r\n",
        "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\r\n",
        "    return modelPerformance\r\n",
        " \r\n",
        "result = bestModel(modelsWithParam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "didqmTxir5qw"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRDBUBT2r6B4"
      },
      "source": [
        "#bayesian search cv\r\n",
        "def modelBestFit(item):\r\n",
        "    model = item['model']\r\n",
        "    paramGrid = item['param']\r\n",
        "    search = BayesSearchCV(model, paramGrid,n_jobs=-1,pre_dispatch='1*n_jobs'\r\n",
        "                           ,scoring = scoring,refit = True,cv = fold)\r\n",
        "    search.fit(X_train,y_train)\r\n",
        "    best_score =search.score(X_test,y_test)\r\n",
        "    model_name = model.__class__.__name__\r\n",
        "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\r\n",
        "\r\n",
        "def bestModel(modelsAndParams):\r\n",
        "    modelPerformance = pd.DataFrame()\r\n",
        "    for item in modelsAndParams:\r\n",
        "        result = modelBestFit(item)\r\n",
        "        \r\n",
        "        modelPerformance = modelPerformance.append(result,ignore_index=True)\r\n",
        "        \r\n",
        "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\r\n",
        "    return modelPerformance\r\n",
        " \r\n",
        "result = bestModel(modelsWithParam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w3bwXWkr8CO"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}