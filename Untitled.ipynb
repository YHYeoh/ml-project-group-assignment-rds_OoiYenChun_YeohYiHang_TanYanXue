{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV \n",
    "from sklearn.linear_model import Perceptron, LogisticRegressionCV, RidgeClassifierCV,LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, Lasso,LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score,mean_absolute_error, confusion_matrix, silhouette_score\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc, classification_report,precision_score,recall_score,log_loss,f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, LabelEncoder, OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer,minmax_scale,PolynomialFeatures\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn import tree\n",
    "import pandas_bokeh\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from numpy import mean, std\n",
    "import pandas.testing as tm\n",
    "from scipy import stats\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "\n",
    "from yellowbrick.features import PCA as PCA_YB\n",
    "from yellowbrick.features.radviz import RadViz\n",
    "from yellowbrick.features import pca_decomposition\n",
    "from yellowbrick.features import Manifold\n",
    "from yellowbrick.features import JointPlotVisualizer\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "from yellowbrick.features import ParallelCoordinates\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "\n",
    "#other\n",
    "from math import sqrt\n",
    "import inspect\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.stats import loguniform, uniform\n",
    "from bokeh import io\n",
    "import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import eli5\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['Basic', 'Graduation', 'Master', '2n Cycle', 'PhD'], dtype=object)]\n",
      "0    1906\n",
      "1     334\n",
      "Name: Response, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "DATASET_URL = \"https://gist.githubusercontent.com/YHYeoh/ad1a7f7170c72d621d05a70637540152/raw/5a6059c199e2c46d2f3d258f03d93cfea98e2749/marketing_campaign.csv\"\n",
    "data = pd.read_csv(DATASET_URL, sep = ';')\n",
    "\n",
    "pd.set_option('plotting.backend','pandas_bokeh')\n",
    "\n",
    "# data.fillna(method = \"ffill\", inplace = True)\n",
    "data.isnull().values.any()\n",
    "\n",
    "\n",
    "education_order = [['Basic', 'Graduation', 'Master', '2n Cycle', 'PhD']]\n",
    "ordinal_encoder = OrdinalEncoder(categories=education_order)\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "\n",
    "imr = IterativeImputer(random_state=42, max_iter=100, min_value=data['Income'].min())\n",
    "imr = imr.fit(data[['Income']])\n",
    "data['Income'] = imr.transform(data[['Income']]).ravel()\n",
    "\n",
    "\n",
    "data[\"Education\"] = (ordinal_encoder.fit_transform(data[\"Education\"].values.reshape(-1, 1))).astype(int)\n",
    "print(ordinal_encoder.categories_)\n",
    "\n",
    "data_copy = data.copy()\n",
    "marital_status_ohe = pd.get_dummies(data[\"Marital_Status\"],prefix=\"Marital\")\n",
    "ohe_cols = marital_status_ohe.columns\n",
    "data = pd.concat([data, marital_status_ohe], axis=1)\n",
    "\n",
    "data.drop([\"ID\", 'Dt_Customer',\"Z_CostContact\",\"Z_Revenue\",\"Marital_Status\"], axis=1, inplace=True)\n",
    "\n",
    "categorical = ['Marital_Status']\n",
    "numerical = ['Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n",
    "       'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n",
    "       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\n",
    "       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',\n",
    "       'Complain']\n",
    "numerical_no_bool = ['Education','Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
    "\n",
    "y = data.Response\n",
    "print(y.value_counts())\n",
    "X = data.drop(['Response'], axis=1)\n",
    "X.drop(['NumStorePurchases','NumCatalogPurchases','MntFruits','MntFishProducts','MntSweetProducts','MntWines','Year_Birth'], axis = 1, inplace = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "def acc_model(params):\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    return cross_val_score(clf, X_train, Y_train).mean()\n",
    "\n",
    "param_space = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,150)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100,500)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])}\n",
    "\n",
    "best = 0\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = acc_model(params)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "    print ('new best:', best, params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, param_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print ('best:')\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    t = params['model']\n",
    "    del params['type']\n",
    "    if t == :\n",
    "        clf = BernoulliNB(**params)\n",
    "    elif t == 'svm':\n",
    "        clf = SVC(**params)\n",
    "    elif t == 'dtree':\n",
    "        clf = DecisionTreeClassifier(**params)\n",
    "    elif t == 'knn':\n",
    "        clf = KNeighborsClassifier(**params)\n",
    "    else:\n",
    "        return 0\n",
    "    return cross_val_score(clf, X, y).mean()\n",
    "\n",
    "space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'naive_bayes',\n",
    "        'alpha': hp.uniform('alpha', 0.0, 2.0)\n",
    "    },\n",
    "    {\n",
    "        'type': 'svm',\n",
    "        'C': hp.uniform('C', 0, 10.0),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
    "        'gamma': hp.uniform('gamma', 0, 20.0)\n",
    "    },\n",
    "    {\n",
    "        'type': 'randomforest',\n",
    "        'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "        'max_features': hp.choice('max_features', range(1,5)),\n",
    "        'n_estimators': hp.choice('n_estimators', range(1,20)),\n",
    "        'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "        'scale': hp.choice('scale', [0, 1]),\n",
    "        'normalize': hp.choice('normalize', [0, 1])\n",
    "    },\n",
    "    {\n",
    "        'type': 'knn',\n",
    "        'n_neighbors': hp.choice('knn_n_neighbors', range(1,50))\n",
    "    }\n",
    "])\n",
    "\n",
    "count = 0\n",
    "best = 0\n",
    "def f(params):\n",
    "    global best, count\n",
    "    count += 1\n",
    "    acc = hyperopt_train_test(params.copy())\n",
    "    if acc > best:\n",
    "        print 'new best:', acc, 'using', params['type']\n",
    "        best = acc\n",
    "    if count % 50 == 0:\n",
    "        print 'iters:', count, ', acc:', acc, 'using', params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, space, algo=tpe.suggest, max_evals=1500, trials=trials)\n",
    "print 'best:'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsWithParam = [\n",
    "    {\n",
    "        'name':'rfc','model':RandomForestClassifier(),'param':{\n",
    "            'bootstrap': [True, False],\n",
    "             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': [1, 2, 4],\n",
    "             'min_samples_split': [2, 5, 10],\n",
    "             'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "    },\n",
    "{\n",
    "        'model':LGBMClassifier(),'param':{\n",
    "        'boosting_type':[\"gbdt\",\"dart\",\"goss\",\"rf\"],\n",
    "        'metric':['binary_logloss'],\n",
    "        'sub_feature':list(np.arange(0.1,1,10)),\n",
    "        'num_leaves':list(range(10,50,10)),\n",
    "        'learning_rate': [1,0.1,0.01,0.005,0.001,0.0005,0.0001],\n",
    "        'n_estimators': list(range(100,1000,100)),\n",
    "        'min_data':[50],\n",
    "        'max_depth': list(range(5,20,5)),\n",
    "        'min_split_gain':list(np.arange(0.1,1,10)),\n",
    "        'random_state': [42]\n",
    "    }},{\n",
    "            \n",
    "        'model':SVC(),'param':{\n",
    "        'C': [ 1,0.1,10, 100, 1000],  \n",
    "        'gamma': [1,0.1,0.01,0.005,0.001,0.0005,0.0001], \n",
    "        'kernel': ['linear', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'random_state': [42] \n",
    "    }},\n",
    "    { #split svc into 2 as svc poly and non-poly kernel has\n",
    "        #different parameter\n",
    "        'model':SVC(),'param':{\n",
    "        'C': [0.1, 1, 10, 100, 1000],  \n",
    "        'gamma': [1,0.1,0.01,0.005,0.001,0.0005,0.0001], \n",
    "        'kernel': ['poly'],\n",
    "        'degree':list(range(3,10)),\n",
    "        'random_state': [42],\n",
    "    }},{#split logistic regression into as saga solver has different parameter needs\n",
    "        'model':LogisticRegression(),'param':{'Cs': [[100,10,1, 0.1,0.05,0.001,0.0001]],#100,10,1, 0.1, 0.01, 0.001\n",
    "                                 'fit_intercept':[True,False],\n",
    "                                 'normalize':[True,False],\n",
    "                                 'dual':[True,False],\n",
    "                                 'penalty':['L2'],\n",
    "                                 'penalty':[True],\n",
    "                                 'max_iter':list(range(100,1000,100)),#[50,100,500,1000,2000,4000,8000]\n",
    "                                 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "                                 'random_state':[42]\n",
    "        }},\n",
    "        {\n",
    "        'model':LogisticRegression(),'param':{'Cs': [[100,10,1, 0.1,0.05,0.001,0.0001]],#100,10,1, 0.1, 0.01, 0.001\n",
    "                                 'fit_intercept':[True,False],\n",
    "                                 'normalize':[True,False],\n",
    "                                 'penalty':['elasticnet'],\n",
    "                                 'penalty':[True],\n",
    "                                 'max_iter':list(range(100,1000,100)),#[50,100,500,1000,2000,4000,8000]\n",
    "                                 'solver':['saga'],\n",
    "                                 'random_state':[42]\n",
    "        }}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log sync requires rsync to be installed.\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "The `start_trial` operation took 0.515 s, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "The `on_step_begin` operation took 0.755 s, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "The `process_trial_result` operation took 0.899 s, which may be a performance bottleneck.\n",
      "Processing trial results took 0.900 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "The `process_trial` operation took 0.901 s, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n",
      "\u001b[2m\u001b[36m(pid=5512)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=5512)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6880)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=6880)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-17 23:59:47,821\tINFO services.py:1174 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-04-17 23:59:53,000\tWARNING trial_runner.py:420 -- Trial Runner checkpointing failed: can't pickle dict_values objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=4308)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Fatal] Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m [LightGBM] [Fatal] Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\u001b[2m\u001b[36m(pid=968)\u001b[0m \n",
      "2021-04-18 00:00:00,818\tERROR trial_runner.py:613 -- Trial _Trainable_f61f059e: Error processing event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11936)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "ename": "RayTaskError(LightGBMError)",
     "evalue": "\u001b[36mray::_Trainable.train_buffered()\u001b[39m (pid=968, ip=192.168.0.116)\n  File \"python\\ray\\_raylet.pyx\", line 480, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\function_manager.py\", line 556, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trainable.py\", line 167, in train_buffered\n    result = self.train()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trainable.py\", line 226, in train\n    result = self.step()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\_trainable.py\", line 106, in step\n    return self._train()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\_trainable.py\", line 246, in _train\n    error_score=\"raise\")\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 252, in cross_validate\n    for train, test in cv.split(X, y, groups))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 222, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\sklearn.py\", line 857, in fit\n    callbacks=callbacks, init_model=init_model)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\sklearn.py\", line 617, in fit\n    callbacks=callbacks, init_model=init_model)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\basic.py\", line 2061, in __init__\n    ctypes.byref(self.handle)))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\nlightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError(LightGBMError)\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-61e53de970fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodelPerformance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelsWithParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-61e53de970fa>\u001b[0m in \u001b[0;36mbestModel\u001b[1;34m(modelsAndParams)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodelsAndParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelBestFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-61e53de970fa>\u001b[0m in \u001b[0;36mmodelBestFit\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m     15\u001b[0m        search_optimization=\"bayesian\",refit=True)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#need to make sure the score is done by best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\tune_basesearch.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    662\u001b[0m                                     \"To show process output, set verbose=2.\")\n\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mray_init\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\tune_basesearch.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fill_config_hyperparam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         \u001b[0manalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tune_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresources_per_trial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\tune_search.py\u001b[0m in \u001b[0;36m_tune_run\u001b[1;34m(self, config, resources_per_trial)\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fail_fast='raise' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                 \"detected.\")\n\u001b[1;32m--> 715\u001b[1;33m             \u001b[0manalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mrun_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\tune.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_staging_grace_period\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"process_trial\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[1;31m# `self._queued_trial_decisions` now contains a final decision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36m_process_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m    585\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             with warn_if_slow(\n\u001b[0;32m    588\u001b[0m                     \u001b[1;34m\"process_trial_results\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\u001b[0m in \u001b[0;36mfetch_result\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_running\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_future\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fetch_result\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_future\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEFAULT_GET_TIMEOUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;31m# For local mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclient_mode_enabled\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_client_hook_enabled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\worker.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   1454\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1455\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1456\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1457\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRayTaskError(LightGBMError)\u001b[0m: \u001b[36mray::_Trainable.train_buffered()\u001b[39m (pid=968, ip=192.168.0.116)\n  File \"python\\ray\\_raylet.pyx\", line 480, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\function_manager.py\", line 556, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trainable.py\", line 167, in train_buffered\n    result = self.train()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trainable.py\", line 226, in train\n    result = self.step()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\_trainable.py\", line 106, in step\n    return self._train()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\_trainable.py\", line 246, in _train\n    error_score=\"raise\")\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 252, in cross_validate\n    for train, test in cv.split(X, y, groups))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 222, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\sklearn.py\", line 857, in fit\n    callbacks=callbacks, init_model=init_model)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\sklearn.py\", line 617, in fit\n    callbacks=callbacks, init_model=init_model)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\basic.py\", line 2061, in __init__\n    ctypes.byref(self.handle)))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\nlightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 ."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] feature_fraction is set with colsample_bytree=1.0, will be overridden by sub_feature=0.1. Current value: feature_fraction=0.1\n",
      "\u001b[2m\u001b[36m(pid=11744)\u001b[0m [LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_data=50. Current value: min_data_in_leaf=50\n"
     ]
    }
   ],
   "source": [
    "#bayesian optimised search cv\n",
    "import ray\n",
    "from tune_sklearn import TuneSearchCV\n",
    "fold=10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def modelBestFit(item):\n",
    "    model = item['model']\n",
    "    paramGrid = item['param']\n",
    "    search = TuneSearchCV(model,\n",
    "       param_distributions=paramGrid,         \n",
    "       n_jobs=-1,\n",
    "       random_state=42,\n",
    "       search_optimization=\"bayesian\",refit=True)\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    print(search.best_params_)\n",
    "    #need to make sure the score is done by best model\n",
    "    best_score =search.score(X_test,y_test.values)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    return {'model_name':model_name,'best_score':best_score,'best_model':search.best_estimator_}\n",
    "\n",
    "def bestModel(modelsAndParams):\n",
    "    modelPerformance = pd.DataFrame()\n",
    "    count=0\n",
    "    for item in modelsAndParams:\n",
    "        count=count+1\n",
    "        result = modelBestFit(item)\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "        modelPerformance = modelPerformance.append(result,ignore_index=True)\n",
    "        \n",
    "    modelPerformance.sort_values(by='best_score',ascending=False,inplace=True)\n",
    "    return modelPerformance\n",
    " \n",
    "result = bestModel(modelsWithParam)\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "Log sync requires rsync to be installed.\n",
      "Trial Runner checkpointing failed: can't pickle dict_values objects\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-86c4c6a26967>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodelPerformance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelsWithParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-86c4c6a26967>\u001b[0m in \u001b[0;36mbestModel\u001b[1;34m(modelsAndParams)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodelsAndParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelBestFit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-86c4c6a26967>\u001b[0m in \u001b[0;36mmodelBestFit\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m     12\u001b[0m        search_optimization=\"bayesian\")\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#need to make sure the score is done by best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\tune_basesearch.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    662\u001b[0m                                     \"To show process output, set verbose=2.\")\n\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mray_init\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\tune_basesearch.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fill_config_hyperparam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         \u001b[0manalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tune_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresources_per_trial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\tune_sklearn\\tune_search.py\u001b[0m in \u001b[0;36m_tune_run\u001b[1;34m(self, config, resources_per_trial)\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fail_fast='raise' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                 \"detected.\")\n\u001b[1;32m--> 715\u001b[1;33m             \u001b[0manalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mrun_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0manalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\tune.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_staging_grace_period\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[1;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             trial = self.trial_executor.get_next_available_trial(\n\u001b[1;32m--> 523\u001b[1;33m                 timeout=timeout)  # blocking\n\u001b[0m\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclient_mode_enabled\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_client_hook_enabled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\ray\\worker.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[0;32m   1596\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m             \u001b[0mworker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m             \u001b[0mfetch_local\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1599\u001b[0m         )\n\u001b[0;32m   1600\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpython\\ray\\_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpython\\ray\\_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_sklearn import TuneSearchCV\n",
    "import warnings\n",
    "# Other imports\n",
    "import scipy\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Set training and validation sets\n",
    "X, y = make_classification(n_samples=11000, n_features=1000, n_informative=50, \n",
    "                           n_redundant=0, n_classes=10, class_sep=2.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n",
    "\n",
    "# Example parameter distributions to tune from SGDClassifier\n",
    "# Note the use of tuples instead if Bayesian optimization is desired\n",
    "param_dists = {\n",
    "   'alpha': (1e-4, 1e-1),\n",
    "   'epsilon': (1e-2, 1e-1)\n",
    "}\n",
    "\n",
    "tune_search = TuneSearchCV(SGDClassifier(),\n",
    "   param_distributions=param_dists,\n",
    "   early_stopping=True,\n",
    "   max_iters=10,\n",
    "   search_optimization=\"bayesian\"\n",
    ")\n",
    "warnings.filterwarnings(\"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # import libraries\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # estimator list for model-specific feature importance techniques\n",
    "# estimators = [\n",
    "#     LogisticRegression,\n",
    "#     XGBClassifier,\n",
    "#     RandomForestClassifier,\n",
    "#     KNeighborsClassifier,\n",
    "# ]\n",
    "\n",
    "# # import libraries\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # estimator list for model-specific feature importance techniques\n",
    "# estimators = [\n",
    "#     LogisticRegression,\n",
    "#     XGBClassifier,\n",
    "#     RandomForestClassifier,\n",
    "#     KNeighborsClassifier,\n",
    "# ]\n",
    "\n",
    "# # instantiate FeatureSelector object\n",
    "# fs = mlmachine_titanic_train.FeatureSelector(\n",
    "#     data=mlmachine_titanic_train.data,\n",
    "#     target=mlmachine_titanic_train.target,\n",
    "#     estimators=estimators,\n",
    "# )\n",
    "\n",
    "# # run full feature selector suite, use accuracy metric and \n",
    "# # 0 CV folds where applicable\n",
    "# feature_selector_summary = fs.feature_selector_suite(\n",
    "#     sequential_scoring=\"accuracy\",\n",
    "#     sequential_n_folds=0,\n",
    "#     add_stats=True,\n",
    "#     n_jobs=1,\n",
    "#     save_to_csv=True,\n",
    "# )\n",
    "# view rawmlmachine_p4_fs.py hosted with  by GitHub\n",
    "\n",
    "# # import libraries\n",
    "# from hyperopt import hp\n",
    "# # \tRidgeClassifierCV(),\n",
    "# # \tSVC(kernel = 'linear',max_iter= -1), \n",
    "# # \tRandomForestClassifier(), #no coef \n",
    "# # \tGradientBoostingClassifier(),#no coef \n",
    "# # \tXGBClassifier(),\n",
    "\n",
    "# # create hyperopt parameter space for set of estimators\n",
    "\n",
    "# view rawmlmachine_p4_space_setup.py hosted with  by GitHub# execute bayesian optimization grid search\n",
    "# mlmachine_titanic_train.exec_bayes_optim_search(\n",
    "#     estimator_parameter_space=estimator_parameter_space,\n",
    "#     data=mlmachine_titanic_train.data,\n",
    "#     target=mlmachine_titanic_train.target,\n",
    "#     columns=cross_val_feature_dict,\n",
    "#     scoring=\"accuracy\",\n",
    "#     n_folds=5,\n",
    "#     n_jobs=5,\n",
    "#     iters=200,\n",
    "#     show_progressbar=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes_optim_summary = pd.read_csv(\"data/bayes_optimization_summary_accuracy.csv\", na_values=\"nan\")\n",
    "# bayesTrain = pd.concat([bayesx_train, bayesy_train], axis=1)\n",
    "# bayesValid = pd.concat([bayesx_test], axis=1)\n",
    "\n",
    "# # import libraries\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # import mlmachine tools\n",
    "# import mlmachine as mlm\n",
    "# df_train, df_valid = bayesTrain,bayesValid\n",
    "# # age is ordinal\n",
    "# # ordinal encoding hierarchy\n",
    "# ordinal_encodings = {\"Education\": [1, 2, 3,4]}\n",
    "\n",
    "# # instantiate a Machine object for the training data\n",
    "# mlmachine_titanic_train = mlm.Machine(\n",
    "#     data=df_train,\n",
    "#     target=\"Response\",\n",
    "#     identify_as_continuous=[\"Income\",\"MntMeatProducts\",\"MntGoldProds\"],\n",
    "#     identify_as_count=[\"Kidhome\",\"Teenhome\",\"NumDealsPurchases\"\n",
    "#                        ,\"NumWebPurchases\",\"NumWebVisitsMonth\",\"Recency\"],\n",
    "#     identify_as_nominal=[\"AcceptedCmp3\",\"AcceptedCmp4\",\"AcceptedCmp5\"\n",
    "#                          ,\"AcceptedCmp1\",\"AcceptedCmp2\",\"Complain\"\n",
    "#                          ,\"Marital_Absurd\",\"Marital_Alone\",\"Marital_Single\"\n",
    "#                          ,\"Marital_Divorced\",\"Marital_Married\",\"Marital_Together\",\n",
    "#                         \"Marital_Widow\",\"Marital_YOLO\"],\n",
    "#     identify_as_ordinal=[\"Education\"],\n",
    "#     ordinal_encodings=ordinal_encodings,\n",
    "#     is_classification=True\n",
    "# )\n",
    "\n",
    "# # instantiate a Machine object for the validation data\n",
    "# mlmachine_titanic_valid = mlm.Machine(\n",
    "#     data=df_valid,\n",
    "#     identify_as_continuous=[\"Income\",\"MntMeatProducts\",\"MntGoldProds\"],\n",
    "#     identify_as_count=[\"Kidhome\",\"Teenhome\",\"NumDealsPurchases\"\n",
    "#                        ,\"NumWebPurchases\",\"NumWebVisitsMonth\",\"Recency\"],\n",
    "#     identify_as_nominal=[\"AcceptedCmp3\",\"AcceptedCmp4\",\"AcceptedCmp5\"\n",
    "#                          ,\"AcceptedCmp1\",\"AcceptedCmp2\",\"Complain\"\n",
    "#                          ,\"Marital_Absurd\",\"Marital_Alone\",\"Marital_Single\"\n",
    "#                          ,\"Marital_Divorced\",\"Marital_Married\",\"Marital_Together\",\n",
    "#                         \"Marital_Widow\",\"Marital_YOLO\"],\n",
    "#     identify_as_ordinal=[\"Education\"],\n",
    "#     ordinal_encodings=ordinal_encodings,\n",
    "#     is_classification=True\n",
    "# )\n",
    "# from hyperopt import hp\n",
    "# estimator_parameter_space = {\n",
    "#     \"LogisticRegression\": {\n",
    "#         \"C\": hp.loguniform(\"C\", np.log(0.001), np.log(0.2)),\n",
    "#         \"penalty\": hp.choice(\"penalty\", [\"l2\"]),\n",
    "# #         \"solver\": hp.choice(\"solver\", [\"liblinear\",\"saga\",\"lbfgs\",\"sag\",\"newton-cg\"]),\n",
    "#     },\n",
    "#     \"XGBClassifier\": {\n",
    "#         \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "#         \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "#         \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "#         \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "#         \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "#         \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "#         \"subsample\": hp.uniform(\"subsample\", 0.3, 1),\n",
    "#     },\n",
    "#     \"RandomForestClassifier\": {\n",
    "#         \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "#         \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "#         \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "#         \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\",\"log2\",None]),\n",
    "#         \"min_samples_split\": hp.choice(\n",
    "#             \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "#         ),\n",
    "#         \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "#     },\n",
    "#     \"LGBMClassifier\": {\n",
    "#         'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "#         'boosting_type': hp.choice('boosting_type',\n",
    "#                                    [{'boosting_type': 'gbdt',\n",
    "# #                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n",
    "#                                      },\n",
    "#                                     {'boosting_type': 'dart',\n",
    "# #                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n",
    "#                                      },\n",
    "#                                     {'boosting_type': 'goss'}]),\n",
    "#         'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "#         'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "#         'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n",
    "#         'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1), #alias \"subsample\"\n",
    "#         'min_data_in_leaf': hp.qloguniform('min_data_in_leaf', 0, 6, 1),\n",
    "#         'lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('lambda_l1_positive', -16, 2)]),\n",
    "#         'lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('lambda_l2_positive', -16, 2)]),\n",
    "#         'verbose': -1,\n",
    "#         'subsample': None, \n",
    "#         'reg_alpha': None, \n",
    "#         'reg_lambda': None,\n",
    "#         'min_sum_hessian_in_leaf': None,\n",
    "#         'min_child_samples': None,\n",
    "#         'colsample_bytree': None,\n",
    "#         'min_child_weight': hp.loguniform('min_child_weight', -16, 5)\n",
    "#     },\n",
    "#     \"SVC\": {\n",
    "#         'C': hp.uniform('C', 0, 10.0),\n",
    "#         'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
    "#         'gamma': hp.uniform('gamma', 0, 20.0)\n",
    "#     }\n",
    "    \n",
    "# }\n",
    "# mlmachine_titanic_train.exec_bayes_optim_search(\n",
    "#     estimator_parameter_space=estimator_parameter_space,\n",
    "#     data=mlmachine_titanic_train.data,\n",
    "#     target=mlmachine_titanic_train.target,\n",
    "#     columns=df_train.columns,\n",
    "#     scoring=\"accuracy\",\n",
    "#     n_folds=5,\n",
    "#     n_jobs=5,\n",
    "#     iters=200,\n",
    "#     show_progressbar=True,\n",
    "# )\n",
    "# digits = datasets.load_digits()\n",
    "# X = digits.data\n",
    "# y = digits.target\n",
    "# print X.shape, y.shape\n",
    "# def hyperopt_train_test(params):\n",
    "#     t = params['type']\n",
    "#     del params['type']\n",
    "#     if t == 'naive_bayes':\n",
    "#         clf = BernoulliNB(**params)\n",
    "#     elif t == 'svm':\n",
    "#         clf = SVC(**params)\n",
    "#     elif t == 'dtree':\n",
    "#         clf = DecisionTreeClassifier(**params)\n",
    "#     elif t == 'knn':\n",
    "#         clf = KNeighborsClassifier(**params)\n",
    "#     else:\n",
    "#         return 0\n",
    "#     return cross_val_score(clf, X, y).mean()\n",
    "# space = hp.choice('classifier_type', [\n",
    "#     {\n",
    "#         'type': 'naive_bayes',\n",
    "#         'alpha': hp.uniform('alpha', 0.0, 2.0)\n",
    "#     },\n",
    "#     {\n",
    "#         'type': 'svm',\n",
    "#         'C': hp.uniform('C', 0, 10.0),\n",
    "#         'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
    "#         'gamma': hp.uniform('gamma', 0, 20.0)\n",
    "#     },\n",
    "#     {\n",
    "#         'type': 'randomforest',\n",
    "#         'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "#         'max_features': hp.choice('max_features', range(1,5)),\n",
    "#         'n_estimators': hp.choice('n_estimators', range(1,20)),\n",
    "#         'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "#         'scale': hp.choice('scale', [0, 1]),\n",
    "#         'normalize': hp.choice('normalize', [0, 1])\n",
    "#     },\n",
    "#     {\n",
    "#         'type': 'knn',\n",
    "#         'n_neighbors': hp.choice('knn_n_neighbors', range(1,50))\n",
    "#     }\n",
    "# ])\n",
    "# count = 0\n",
    "# best = 0\n",
    "\n",
    "# def f(params):\n",
    "#     global best, count\n",
    "#     count += 1\n",
    "#     acc = hyperopt_train_test(params.copy())\n",
    "#     if acc > best:\n",
    "#         print 'new best:', acc, 'using', params['type']\n",
    "#         best = acc\n",
    "#     if count % 50 == 0:\n",
    "#         print 'iters:', count, ', acc:', acc, 'using', params\n",
    "#     return {'loss': -acc, 'status': STATUS_OK}\n",
    "# trials = Trials()\n",
    "# best = fmin(f, space, algo=tpe.suggest, max_evals=1500, trials=trials)\n",
    "# print 'best:'\n",
    "# print best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
