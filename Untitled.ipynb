{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV \n",
    "from sklearn.linear_model import Perceptron, LogisticRegressionCV, RidgeClassifierCV, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score,mean_absolute_error, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc, classification_report,precision_score,recall_score,log_loss,f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, LabelEncoder, OneHotEncoder, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "import pandas_bokeh\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from numpy import mean, std\n",
    "import pandas.testing as tm\n",
    "from scipy import stats\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#other\n",
    "from math import sqrt\n",
    "import inspect\n",
    "\n",
    "import eli5\n",
    "#%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasmethod(obj, name):\n",
    "    return inspect.ismethod(getattr(obj, name, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y, y_hat, title = 'Confusion Matrix'):\n",
    "    cm = confusion_matrix(y, y_hat)\n",
    "    precision = precision_score(y, y_hat)\n",
    "    recall = recall_score(y, y_hat)\n",
    "    accuracy = accuracy_score(y,y_hat)\n",
    "    f1 = f1_score(y,y_hat)\n",
    "    print('Recall: ', recall)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('Precision: ', precision)\n",
    "    print('F1: ', f1)\n",
    "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\n",
    "    plt.xlabel('predicted', fontsize=18)\n",
    "    plt.ylabel('actual', fontsize=18)\n",
    "    plt.title(title, fontsize=18)\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics_summary(y_test,y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall =  recall_score(y_test, y_pred) #sensitivity\n",
    "    specificity = tn / (tn+fp)\n",
    "    g_mean= sqrt(recall * specificity)\n",
    "    mse =mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2=r2_score(y_test, y_pred)\n",
    "    ros = roc_auc_score(y_test, y_pred)\n",
    "    ll = log_loss(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    metrics_collection_dict ={\n",
    "        'accuracy':['accuracy',accuracy],\n",
    "        'precision':['precision',precision],\n",
    "        'recall':['recall',recall],\n",
    "        'specificity':['specificity',specificity],\n",
    "        'g_mean':['g_mean',g_mean],\n",
    "        'mean_square_error':['mean_square_error',mse],\n",
    "        'r2':['r2',r2],\n",
    "        'roc_auc_score':['roc_auc_score',ros],\n",
    "        'log_loss':['log_loss',ll],\n",
    "        'f1_score':['f1_score',f1]\n",
    "    } \n",
    "    metrics_collection=pd.DataFrame.from_dict(metrics_collection_dict,orient='index',columns=['metric','score'])\n",
    "    plt.title(model.__class__.__name__)\n",
    "    ax = sns.barplot(data=metrics_collection.reset_index(), x = 'metric',y=\"score\")\n",
    "    for item in ax.get_xticklabels():\n",
    "        item.set_rotation(60)\n",
    "    plt.show()\n",
    "    return metrics_collection_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "\n",
    "def ROC_Curve_Plot(model,X_test,y_test):\n",
    "    predProb = model.predict_proba(X_test)\n",
    "    preds = predProb[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds,pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.close()\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    #plt.xlim([0, 1])\n",
    "    #plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    return fpr,tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-31-f0b2bf3a34b2>, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-f0b2bf3a34b2>\"\u001b[1;36m, line \u001b[1;32m90\u001b[0m\n\u001b[1;33m    for i,v in enumerate(importance):\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "DATASET_URL = \"https://gist.githubusercontent.com/YHYeoh/ad1a7f7170c72d621d05a70637540152/raw/5a6059c199e2c46d2f3d258f03d93cfea98e2749/marketing_campaign.csv\"\n",
    "data = pd.read_csv(DATASET_URL, sep = ';')\n",
    "\n",
    "pd.set_option('plotting.backend','pandas_bokeh')\n",
    "\n",
    "data.fillna(method = \"ffill\", inplace = True)\n",
    "data.isnull().values.any()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Education\"] = label_encoder.fit_transform(data[\"Education\"])\n",
    "\n",
    "data['enroll_year'] = pd.DatetimeIndex(data.Dt_Customer).year\n",
    "data['enroll_month'] = pd.DatetimeIndex(data.Dt_Customer).month\n",
    "data['enroll_day'] = pd.DatetimeIndex(data.Dt_Customer).day\n",
    "\n",
    "data.drop([\"ID\", 'Dt_Customer',\"Z_CostContact\",\"Z_Revenue\"], axis=1, inplace=True)\n",
    "\n",
    "categorical = ['Marital_Status']\n",
    "numerical = ['Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n",
    "       'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n",
    "       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\n",
    "       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',\n",
    "       'Complain', 'enroll_year', 'enroll_month', 'enroll_day']\n",
    "numerical_no_bool = ['Education','Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth','enroll_day','enroll_month','enroll_year']\n",
    "ss = Pipeline(steps=[('scalers',StandardScaler())])\n",
    "ohe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "\n",
    "models_coef = [LogisticRegressionCV(max_iter= 1200), \n",
    "          RidgeClassifierCV(),\n",
    "          SVC(kernel = 'linear',max_iter= -1), \n",
    "          Perceptron(),\n",
    "          PassiveAggressiveClassifier(), \n",
    "         ]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('cont', ss, numerical_no_bool),\n",
    "                        ('cat', ohe, categorical),\n",
    "                        #('le', le, ordinal),\n",
    "                        ],remainder='passthrough')\n",
    "\n",
    "y = data.Response\n",
    "X = data.drop(['Response'], axis=1)\n",
    "print(\"Features: \" + str(len(X.columns)))\n",
    "print(X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.3)\n",
    "\n",
    "def feature_importance_coef(classifier, feature_names):\n",
    "\tif (hasattr(classifier,'coef_')):\n",
    "\t\timportance = classifier.coef_[0]\n",
    "\telif (hasattr(classifier,'coefs_')):\n",
    "\t\timportance = classifier.coefs_\n",
    "\telif (hasattr(classifier,'feature_importances_')):\n",
    "\t\timportance = classifier.feature_importances_\n",
    "\telse:\n",
    "\t\tprint(\"Cannot extract feature importance, skipping\")\n",
    "\t\treturn\n",
    "\t\tprint(\"Feature Important coef\")\n",
    "\t\tprint(importance)\n",
    "\t\t#importance = results.importances_mean\n",
    "      # summarize feature importance\n",
    "    \n",
    "\tfor i,v in enumerate(importance):\n",
    "\t\tprint('Feature: %d, Score: %.5f' % (i,v))\n",
    "\tzipped = zip(feature_names, importance)\n",
    "\tdf = pd.DataFrame(zipped, columns=[\"feature\", \"value\"])\n",
    "\t# Sort the features by the absolute value of their coefficient\n",
    "\tdf[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\n",
    "\tdf[\"colors\"] = df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\n",
    "\tdf = df.sort_values(\"abs_value\", ascending=False)\n",
    "\t# plot feature importance\n",
    "\tfig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "\tsns.barplot(x=\"feature\",\n",
    "\t            y=\"value\",\n",
    "\t            data=df.head(20),\n",
    "\t           palette=df.head(20)[\"colors\"])\n",
    "\tplt.gcf().subplots_adjust(bottom=0.30)\n",
    "\tax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=14)\n",
    "\tax.set_title(\"Top 20 Features for {}\".format(classifier.__class__.__name__), fontsize=25)\n",
    "\tax.set_ylabel(\"Coef\", fontsize=22)\n",
    "\tax.set_xlabel(\"Feature Name\", fontsize=22)\n",
    "\tplt.show()\n",
    "\n",
    "def feature_importance(classifier, feature_names):\n",
    "    importance = classifier.feature_importances_\n",
    "      # summarize feature importance\n",
    "\tfor i,v in enumerate(importance):\n",
    "        print('Feature: %d, Score: %.5f' % (i,v))\n",
    "\t# plot feature importance\n",
    "\tplt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\tplt.bar([x for x in range(len(importance))], importance)\n",
    "\tplt.xticks([x for x in range(len(importance))],[feature_names[x] for x in range(len(importance))],rotation=90)\n",
    "\tplt.title(classifier.__class__.__name__)\n",
    "\tplt.xlabel('Features')\n",
    "\tplt.ylabel('Importance')\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def evaluation(y, y_hat, title):\n",
    "    cm = confusion_matrix(y, y_hat)\n",
    "    precision = precision_score(y, y_hat)\n",
    "    recall = recall_score(y, y_hat)\n",
    "    accuracy = accuracy_score(y,y_hat)\n",
    "    f1 = f1_score(y,y_hat)\n",
    "    print('Recall: ', recall)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('Precision: ', precision)\n",
    "    print('F1: ', f1)\n",
    "    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws=    {'size':20})\n",
    "    plt.xlabel('predicted', fontsize=18)\n",
    "    plt.ylabel('actual', fontsize=18)\n",
    "    plt.title(title, fontsize=18)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "def cross_validate(classifier, cv):\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocess', preprocess),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    mean = []\n",
    "    fpr =None\n",
    "    tpr = None\n",
    "    \n",
    "    for train_ind, val_ind in cv.split(X_train, y_train):\n",
    "        X_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind]\n",
    "        pipeline.fit(X_t, y_t)\n",
    "        y_hat_t = pipeline.predict(X_t)\n",
    "        train_acc.append(accuracy_score(y_t, y_hat_t))\n",
    "        X_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind] \n",
    "        y_hat_val = pipeline.predict(X_val)\n",
    "        test_acc.append(accuracy_score(y_val, y_hat_val))\n",
    "\n",
    "    # ohe_cols = pipeline['preprocess'].transformers_[1][1]['ohe']\\\n",
    "    #                .get_feature_names(categorical)\n",
    "    ohe_cols = list(pipeline.named_steps['preprocess'].named_transformers_['cat'].named_steps['ohe'].get_feature_names(input_features=categorical))\n",
    "    # feature_names = numerical.copy()\n",
    "    # feature_names = [y for x in [feature_names, ohe_cols] for y in x] \n",
    "    # print(len(feature_names))\n",
    "    print(\"ohe length \" + str(len(ohe_cols)))\n",
    "    numeric_feature_list = list(numerical)\n",
    "    # test = numeric_feature_list.extend(ohe_cols)\n",
    "    print(len(numerical))\n",
    "    for i in ohe_cols:\n",
    "    \tnumeric_feature_list.append(i)\n",
    "    print(len(numeric_feature_list))\n",
    "    evaluation(y_val, y_hat_val, 'Confusion Matrix | {}'.format(classifier.__class__.__name__))\n",
    "    metrics_summ = metrics_summary(y_val,y_hat_val)\n",
    "    if hasmethod(pipeline['classifier'], 'predict_proba'):\n",
    "        fpr,tpr = ROC_Curve_Plot(pipeline,X_val,y_val)\n",
    "    print('Mean Training Accuracy: {} | Standard Deviation: {}'.format(np.mean(train_acc),np.std(test_acc)))\n",
    "    print('Mean Validation Accuracy: {} | Standard Deviation: {}'.format(np.mean(test_acc),np.std(test_acc)))\n",
    "    print('\\n')\n",
    "    feature_importance_coef(classifier, numeric_feature_list)\n",
    "    return metrics_summ, fpr,tpr\n",
    "  \n",
    "\n",
    "# for model in models_coef:\n",
    "#   print(model.__class__.__name__)\n",
    "#   cross_validate(model,KFold())\n",
    "\n",
    "\n",
    "models_feature_importance = [ \n",
    "\t#CalibratedClassifierCV() #use to calibrate existing classifier \n",
    "\tDecisionTreeClassifier(), #no coef \n",
    "\tKNeighborsClassifier(),#no feat_import, use permutation_importance \n",
    "\tGaussianNB(), #no feat_import, use permutation_importance \n",
    "\tLGBMClassifier(),#no coef \n",
    "\tRandomForestClassifier(), #no coef \n",
    "\tGradientBoostingClassifier(),#no coef \n",
    "\tPassiveAggressiveClassifier(), \n",
    "\tExtraTreesClassifier(), #no coef \n",
    "\tXGBClassifier(),\n",
    "\t#BaggingClassifier(), #no feat_import, ensemble classifier , Voting Classifier \n",
    "\tAdaBoostClassifier(), #no coef \n",
    "\t#GaussianProcessClassifier(),  #no feat_import, special use case\n",
    "\tMLPClassifier()#no feat_import, special use case\n",
    "\t]\n",
    "\n",
    "\n",
    "for model in models_feature_importance:\n",
    "  print(model.__class__.__name__)\n",
    "  cross_validate(model,KFold())\n",
    "#another batch for special classifier\n",
    "\n",
    "# models_permutation = [KNeighborsClassifier(),GaussianNB()]\n",
    "# enc = OneHotEncoder()\n",
    "# enc_df = pd.DataFrame(enc.fit_transform(X[[\"Marital_Status\"]]).toarray())\n",
    "# X = X.join(enc_df)\n",
    "# X.drop([\"Marital_Status\"], axis=1, inplace=True)\n",
    "# columns = X.columns\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "# for model in models_permutation:\n",
    "# \tmodel = KNeighborsClassifier()\n",
    "# \tmodel.fit(X,y)\n",
    "# \t# perform permutation importance\n",
    "# \tresults = permutation_importance(model, X, y, scoring='accuracy')\n",
    "# \t# get importance\n",
    "# \timportance = results.importances_mean\n",
    "# \t# summarize feature importance\n",
    "# \tfor i,v in enumerate(importance):\n",
    "# \t\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# \t# plot feature importance\n",
    "# \tzipped = zip(columns, importance)\n",
    "# \tdf = pd.DataFrame(zipped, columns=[\"feature\", \"value\"])\n",
    "# \t# Sort the features by the absolute value of their coefficient\n",
    "# \tdf[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\n",
    "# \tdf[\"colors\"] = df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\n",
    "# \tdf = df.sort_values(\"abs_value\", ascending=False)\n",
    "# \t# plot feature importance\n",
    "# \tfig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "# \tsns.barplot(x=\"feature\",\n",
    "# \t            y=\"value\",\n",
    "# \t            data=df.head(20),\n",
    "# \t           palette=df.head(20)[\"colors\"])\n",
    "# \tplt.gcf().subplots_adjust(bottom=0.30)\n",
    "# \tax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=14)\n",
    "# \tax.set_title(\"Top 20 Features for {}\".format(model.__class__.__name__), fontsize=25)\n",
    "# \tax.set_ylabel(\"Coef\", fontsize=22)\n",
    "# \tax.set_xlabel(\"Feature Name\", fontsize=22)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = []\n",
    "for model in models:\n",
    "  print(model.__class__.__name__)\n",
    "  performance = cross_validate(model,KFold())\n",
    "  model_approximation = [model.__class__.__name__,performance]\n",
    "  model_result.append(model_approximation) \n",
    "  #grid_search(model,KFold(),print_feat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain nameList\n",
    "nameList = []\n",
    "\n",
    "for model in model_result:\n",
    "    nameList.append(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all metric\n",
    "metric_list = ['accuracy','precision','recall','specificity','g_mean'\n",
    "                   ,'mean_square_error','r2','roc_auc_score','log_loss','f1_score']\n",
    "\n",
    "for metric in metric_list:\n",
    "    resultList = []\n",
    "    for model in model_result:\n",
    "        resultList.append(model[1][0][metric][1])\n",
    "    accDF = pd.DataFrame(list(zip(nameList,resultList)),columns=['trained_model',metric])\n",
    "    plt.title(\"Models' \"+metric)\n",
    "    ax = sns.barplot(data=accDF.sort_values(metric,ascending=False),orient='h',palette =\"Paired\" , y = 'trained_model',x=metric)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot auc_curve for predict_proba supported model\n",
    "result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "for model in model_result:\n",
    "    fpr = model[1][1]\n",
    "    if(fpr is not None):\n",
    "        tpr = model[1][2]\n",
    "        auc = model[1][0]['roc_auc_score'][1]\n",
    "        result_table = result_table.append({'classifiers':model[0],\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)\n",
    "        \n",
    "result_table.set_index('classifiers', inplace=True)\n",
    "\n",
    "\n",
    "fontP = FontProperties()\n",
    "fontP.set_size('large')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend( title='Models', bbox_to_anchor=(1.05, 0.85), loc='upper left', prop=fontP)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outlier in column\n",
    "pd.set_option('max_columns', None)\n",
    "print(data.describe())\n",
    "\n",
    "# Plot scatter plot and box plot\n",
    "sns.boxplot(x=data['Income'])\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(data['Response'], data['Income'])\n",
    "ax.set_xlabel('Response?')\n",
    "ax.set_ylabel('Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to correct the outlier\n",
    "print(data.dtypes.value_counts())\n",
    "data = data[(np.abs(stats.zscore(data[['Income']])) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot again to check result\n",
    "sns.boxplot(x=data['Income'])\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(data['Response'], data['Income'])\n",
    "ax.set_xlabel('Response?')\n",
    "ax.set_ylabel('Income')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
